{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline Model",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "fexOHcCYV9jG",
        "d7drZlMCSsUB",
        "0TtSmxhhSvNK"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqqB9GxC5HjA",
        "colab_type": "text"
      },
      "source": [
        "Step 1:\n",
        "\n",
        "This notebook prepares the Ontonotes data for the model.  It contains a few steps:\n",
        "\n",
        "+ Copy files from google drive location to colab location\n",
        "+ Parse CoNLL format files into csv's containing the phrase number\n",
        "+ Save csv's to colab location\n",
        "\n",
        "Once run, the data is saved for future uses at \\root\\data\\parsed (or file indicated at output_dir).  This seems to be a user dependent directory so will have to be run for each person.  Also the drive_location might need to be changed to the ID for the google drive folder per user.  These files are \n",
        "\n",
        "TODO:\n",
        "+ Find a file location that is shareable across users\n",
        "\n",
        "\n",
        "Step 2:\n",
        "Consume the ontonotes csv's prepared earlier.  Generate embeddings for use in the model.\n",
        "\n",
        "We want to use multiple embedding sources:\n",
        "+ GloVe\n",
        "+ CoVe\n",
        "+ BERT\n",
        "\n",
        "Each of these embedding sources has different vector lengths.  In the case of BERT we need to tokenize data according to how the model expects it, which will probably require a separate tokenizer as well.  Separate models will likely have to be created for each of the embedding sources.\n",
        "\n",
        "Tokenizing will strip out useful information such as:\n",
        "+ uppercase / lowercase / titlecase\n",
        "\n",
        "This information is useful for NER.  We want to preserve this information and attach it to the embeddings.\n",
        "\n",
        "We'll start by focusing on GloVe since it's pretty simple."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeMAhNAtb-3C",
        "colab_type": "text"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD-WjeNNy_GA",
        "colab_type": "code",
        "outputId": "cae510cd-6ecf-42f2-d2bc-86d04bc1bcf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pip install --upgrade wandb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/9a/35c846af421716ce15a2391f37879f343e03a5c706f8075b9f9dfeb7ce1c/wandb-0.8.5-py2.py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 2.8MB/s \n",
            "\u001b[?25hCollecting watchdog>=0.8.3 (from wandb)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/e3/5a55d48a29300160779f0a0d2776d17c1b762a2039b36de528b093b87d5b/watchdog-0.9.0.tar.gz (85kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 24.2MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0 (from wandb)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/44/fc1ce6e0692f09d1bcdbbb5d2bd008acd824505a12dd85a627ef74f44844/GitPython-2.1.12-py2.py3-none-any.whl (452kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 39.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.21.0)\n",
            "Collecting sentry-sdk>=0.4.0 (from wandb)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/38/7f691570ed9e85479dbe4e0959ae223d364693708ba6d293d850b657f1a0/sentry_sdk-0.10.2-py2.py3-none-any.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 27.4MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0 (from wandb)\n",
            "  Downloading https://files.pythonhosted.org/packages/80/d7/2bfc9332e68d3e15ea97b9b1588b3899ad565120253d3fd71c8f7f13b4fe/shortuuid-0.5.0.tar.gz\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: backports.tempfile>=1.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.0)\n",
            "Collecting gql>=0.1.0 (from wandb)\n",
            "  Downloading https://files.pythonhosted.org/packages/aa/9c/2933b7791210e00f5c26a6243198cc03af9132c29cf85e4c22cb007f171e/gql-0.1.0.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.0)\n",
            "Collecting python-dateutil>=2.6.1 (from wandb)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl (226kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 40.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Collecting subprocess32>=3.5.3 (from wandb)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 29.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb) (3.13)\n",
            "Collecting argh>=0.24.1 (from watchdog>=0.8.3->wandb)\n",
            "  Downloading https://files.pythonhosted.org/packages/06/1c/e667a7126f0b84aaa1c56844337bf0ac12445d1beb9c8a6199a7314944bf/argh-0.26.2-py2.py3-none-any.whl\n",
            "Collecting pathtools>=0.1.1 (from watchdog>=0.8.3->wandb)\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting ddt>=1.1.1 (from GitPython>=1.0.0->wandb)\n",
            "  Downloading https://files.pythonhosted.org/packages/cf/f5/f83dea32dc3fb3be1e5afab8438dce73ed587740a2a061ae2ea56e04a36d/ddt-1.2.1-py2.py3-none-any.whl\n",
            "Collecting gitdb2>=2.0.0 (from GitPython>=1.0.0->wandb)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/30/a407568aa8d8f25db817cf50121a958722f3fc5f87e3a6fba1f40c0633e3/gitdb2-2.0.5-py2.py3-none-any.whl (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 23.5MB/s \n",
            "\u001b[?25hCollecting gitdb>=0.6.4 (from GitPython>=1.0.0->wandb)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/95/7e5d7261feb46c0539ac5e451be340ddd64d78c5118f2d893b052c76fe8c/gitdb-0.6.4.tar.gz (400kB)\n",
            "\u001b[K     |████████████████████████████████| 409kB 47.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2019.6.16)\n",
            "Requirement already satisfied, skipping upgrade: backports.weakref in /usr/local/lib/python3.6/dist-packages (from backports.tempfile>=1.0->wandb) (1.0.post1)\n",
            "Collecting graphql-core>=0.5.0 (from gql>=0.1.0->wandb)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/11/bc4a7eb440124271289d93e4d208bd07d94196038fabbe2a52435a07d3d3/graphql_core-2.2.1-py2.py3-none-any.whl (250kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 43.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: promise>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from gql>=0.1.0->wandb) (2.2.1)\n",
            "Collecting smmap2>=2.0.0 (from gitdb2>=2.0.0->GitPython>=1.0.0->wandb)\n",
            "  Downloading https://files.pythonhosted.org/packages/55/d2/866d45e3a121ee15a1dc013824d58072fd5c7799c9c34d01378eb262ca8f/smmap2-2.0.5-py2.py3-none-any.whl\n",
            "Collecting smmap>=0.8.5 (from gitdb>=0.6.4->GitPython>=1.0.0->wandb)\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/aa/b744b3761fff1b10579df996a2d2e87f124ae07b8336e37edc89cc502f86/smmap-0.9.0.tar.gz\n",
            "Collecting rx<3,>=1.6 (from graphql-core>=0.5.0->gql>=0.1.0->wandb)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/0f/5ef4ac78e2a538cc1b054eb86285fe0bf7a5dbaeaac2c584757c300515e2/Rx-1.6.1-py2.py3-none-any.whl (179kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 45.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: watchdog, shortuuid, gql, subprocess32, pathtools, gitdb, smmap\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/1d/d0/04cfe495619be2095eb8d89a31c42adb4e42b76495bc8f784c\n",
            "  Building wheel for shortuuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/eb/fd/69e5177f67b505e44acbd1aedfbe44b91768ee0c4cd5636576\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/8d/65/a3247f500d675d80a01e4d2f0ee44fe99f1faef575bc2a1664\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for gitdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/7e/c5/1093bc36622d7d06ed6520a50280771ebff8c190d92bd402b6\n",
            "  Building wheel for smmap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/a5/09/cb584a95ad332da2bf0265a8b0423c562a99553a3edf148d12\n",
            "Successfully built watchdog shortuuid gql subprocess32 pathtools gitdb smmap\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: argh, pathtools, watchdog, ddt, smmap2, gitdb2, smmap, gitdb, GitPython, sentry-sdk, shortuuid, docker-pycreds, rx, graphql-core, gql, python-dateutil, subprocess32, wandb\n",
            "  Found existing installation: python-dateutil 2.5.3\n",
            "    Uninstalling python-dateutil-2.5.3:\n",
            "      Successfully uninstalled python-dateutil-2.5.3\n",
            "Successfully installed GitPython-2.1.12 argh-0.26.2 ddt-1.2.1 docker-pycreds-0.4.0 gitdb-0.6.4 gitdb2-2.0.5 gql-0.1.0 graphql-core-2.2.1 pathtools-0.1.2 python-dateutil-2.8.0 rx-1.6.1 sentry-sdk-0.10.2 shortuuid-0.5.0 smmap-0.9.0 smmap2-2.0.5 subprocess32-3.5.4 wandb-0.8.5 watchdog-0.9.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO-mWSKC5Fv_",
        "colab_type": "code",
        "outputId": "541650ff-1f20-4e50-870d-81d3e151f8c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import os\n",
        "import time\n",
        "import sys\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import ast\n",
        "from IPython.display import display\n",
        "\n",
        "from shutil import copyfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "\n",
        "from keras.layers import TimeDistributed, Conv1D, Dense, Embedding, Input, Dropout, LSTM, Bidirectional, MaxPooling1D, Flatten, concatenate\n",
        "from keras.initializers import RandomUniform\n",
        "from keras.optimizers import SGD, Nadam\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nkq7QaJRW7Lr",
        "colab_type": "code",
        "outputId": "4b91d15d-336e-42be-c162-abf4f83c63e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "### File configurations\n",
        "drive_dir = \"/content/drive/My Drive/W266_Project/\"\n",
        "data_src = os.path.join(drive_dir,\"ontonotes\")\n",
        "embed_src = os.path.join(drive_dir,\"embeddings\")\n",
        "\n",
        "# cache store\n",
        "cache_dir = os.path.join(drive_dir, \"cache\")\n",
        "embed_store =  os.path.join(cache_dir, 'embed.h5')\n",
        "\n",
        "### Model Parameters\n",
        "wandb.init(project=\"w266-final\", name=\"your custom run name\")\n",
        "config = wandb.config # Config is a variable that holds and saves hyperparameters and inputs\n",
        "config.dropout = 0.2\n",
        "config.recurrent_dropout = 0.25\n",
        "config.char_vocab = len(string.printable)\n",
        "config.char_embedding_dim = 30\n",
        "config.word_length = 52\n",
        "config.conv_size = 3\n",
        "config.conv_filters = 30\n",
        "config.conv_stride = 1\n",
        "config.conv_window = 52\n",
        "config.lstm_state_size = 200\n",
        "\n",
        "config.epochs = 20\n",
        "config.batch_size = 400\n",
        "config.training_size = 900000\n",
        "\n",
        "# embedding to use\n",
        "# 50d vector is consistent with paper\n",
        "embedding_file = \"glove.6B.50d.txt\"\n",
        "\n",
        "# training data\n",
        "train_file = 'onto.train.ner'\n",
        "dev_file = 'onto.development.ner'\n",
        "test_file = 'onto.test.ner'\n",
        "\n",
        "model_dir = os.path.join(drive_dir, 'output')\n",
        "\n",
        "# if loading a pre-trained model set these\n",
        "model_name = \"std_400b_glove50d_full_04-0.0105.h5\"\n",
        "model_load_path = os.path.join(model_dir, model_name + '.h5')\n",
        "INITIAL_EPOCH = 4\n",
        "\n",
        "# else use these\n",
        "model_name = \"std_400b_glove50d_full\"\n",
        "model_path = os.path.join(model_dir, model_name + '_{epoch:02d}-{val_loss:.4f}.h5')\n",
        "model_results_path = os.path.join(model_dir, model_name + '.csv')\n",
        "\n",
        "\n",
        "### Overwrite and refresh saved files\n",
        "# enable these when changing part of the pre-processing routines\n",
        "# by default we don't pre-process each time for performance reasons\n",
        "OVWR_ONTO = False\n",
        "OVWR_DATA = False\n",
        "OVWR_EMBED = False\n",
        "\n",
        "### Preprocessing Parameters\n",
        "UNK_WORD = \"<UNK-WORD>\"\n",
        "PAD_WORD = \"<PAD-WORD>\"\n",
        "\n",
        "UNK_CHAR = \"<UNK-CHAR>\"\n",
        "PAD_CHAR = \"<PAD-CHAR>\"\n",
        "\n",
        "# max number of words in a sentence, pad to this length, might throw an error if the sentence is longer\n",
        "SENTENCE_WIDTH = 256\n",
        "# max number of characters in a word, pad to this length, will truncate if word is too long\n",
        "WORD_WIDTH = 52\n",
        "# symbols to map padding to\n",
        "CHAR_PAD_SYMBOL = PAD_CHAR\n",
        "LABEL_PAD_SYMBOL = 'O'\n",
        "CASE_PAD_SYMBOL = 'other'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://app.wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/cmillsop/w266-final/runs/9tn5ayxf\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
              "        in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAwtPwVhywtg",
        "colab_type": "code",
        "outputId": "6b1b5ca2-6c94-4a76-bd76-617a32334447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!wandb login 235eba5562b3e4de74ca58d14b2ff8b058cb9986"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fexOHcCYV9jG",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwdmu-_rWhpu",
        "colab_type": "code",
        "outputId": "c6d92e19-2e77-46bd-bf2c-efa71c6e2e6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# show files\n",
        "print(os.listdir(data_src))\n",
        "print(os.listdir(embed_src))\n",
        "print(os.listdir(cache_dir))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['archive', 'onto.development.ner.sample', 'onto.development.ner', 'onto.test.ner.sample', 'onto.train.ner.sample', 'onto.test.ner', 'onto.train.ner']\n",
            "['glove.6B.100d.txt', 'glove.6B.200d.txt', 'glove.6B.300d.txt', 'glove.6B.50d.txt', 'readme.md', 'Skip100']\n",
            "['onto_train_nerword.npy', 'onto_train_nerchar.npy', 'onto_train_nercase.npy', 'onto_train_nerlabel.npy', 'onto_development_nerword.npy', 'onto_development_nerchar.npy', 'onto_development_nercase.npy', 'onto_development_nerlabel.npy', 'onto_test_nerword.npy', 'onto_test_nerchar.npy', 'onto_test_nercase.npy', 'onto_test_nerlabel.npy', 'embed.h5']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2r4lsXGrLkV",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCpi-_CPcdZ8",
        "colab_type": "text"
      },
      "source": [
        "## Load Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ5j2-luYnKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# consider using tf.nn.embedding_lookup instead\n",
        "# or maybe nltk.tokenize\n",
        "\n",
        "\n",
        "def get_casing_ix(word):\n",
        "  '''\n",
        "  determines the casing of the word\n",
        "  \n",
        "  returns casing_ix\n",
        "  '''\n",
        "  if word.istitle():\n",
        "    return case_to_ix['title']\n",
        "  elif word.islower():\n",
        "    return case_to_ix['lower']\n",
        "  elif word.isupper():\n",
        "    return case_to_ix['upper']\n",
        "  elif word.isnumeric():\n",
        "    return case_to_ix['numeric']\n",
        "  return case_to_ix['other']\n",
        "\n",
        "def get_word_ix(word):\n",
        "  '''\n",
        "  takes w and returns the index of the word embedding\n",
        "  out of vocabulary terms return the UNK_WORD and the character embeddings\n",
        "  \n",
        "  returns word_ix\n",
        "  '''\n",
        "  w = word.lower()\n",
        "  w_ix = word_to_ix.get(w)\n",
        "  if w_ix is not None:\n",
        "    return w_ix\n",
        "  return word_to_ix[UNK_WORD]\n",
        "\n",
        "def get_char_ix(char):\n",
        "  char_ix = char_to_ix.get(char)\n",
        "  if char_ix is not None:\n",
        "    return char_ix\n",
        "  return char_to_ix[UNK_CHAR]\n",
        "  \n",
        "def create_character_embeddings(words_df):\n",
        "  '''\n",
        "  Optional function to create pre-trained character embeddings from averaged word embeddings.  In the model we generate them from a uniform random distribution and train.\n",
        "  '''\n",
        "  characters = {}\n",
        "  for i, word_vec in enumerate(words_df.reset_index().values):\n",
        "    for char in word_vec[0]:\n",
        "      if char in characters:\n",
        "        characters[char] = [characters[char][0] + word_vec[1:].astype(float), characters[char][1] + 1]\n",
        "      else:\n",
        "        characters[char] = [word_vec[1:].astype(float), 1]\n",
        "\n",
        "  for key in characters:\n",
        "    characters[key] = np.round(characters[key][0]/characters[key][1],6)\n",
        "    \n",
        "def initialize_word_embeddings(file_name, use_cache=True, debug=True, save_cache=True):\n",
        "  loaded = False\n",
        "  df = None\n",
        "  \n",
        "  if use_cache:\n",
        "    try:\n",
        "      print(\"Attempting to load from cache\")\n",
        "      with pd.HDFStore(embed_store, 'r') as store:\n",
        "        words = store[file_name]\n",
        "      words = pd.read_hdf(embed_store, file_name)\n",
        "      loaded=True\n",
        "      print(\"Loaded successfully\")\n",
        "    except:\n",
        "      print(\"Cache loading failed\")\n",
        "      loaded=False\n",
        "  \n",
        "  if not loaded:\n",
        "    words = pd.read_csv(os.path.join(embed_src, embedding_file), sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
        "    # some embeddings come back with word == NaN\n",
        "    words = words[~words.index.isnull()]\n",
        "    # add entries for special tokens\n",
        "    words.loc[UNK_WORD] = [0 for x in words.columns]\n",
        "    words.loc[PAD_WORD] = [0 for x in words.columns]\n",
        "    if save_cache:\n",
        "      with pd.HDFStore(embed_store, 'a') as store:\n",
        "        store[file_name] = words\n",
        "  \n",
        "  word2ix = {word:i for i,word in enumerate(words.index)}\n",
        "  ix2word = {i:word for i,word in enumerate(words.index)}\n",
        "  words = words.to_numpy().astype(float)\n",
        "  \n",
        "  return words, word2ix, ix2word\n",
        "\n",
        "def initialize_character_embeddings(vocab=string.printable):\n",
        "  characters = [x for x in string.printable]\n",
        "  characters += [UNK_CHAR, PAD_CHAR]\n",
        "  char2ix = {ch:i for i, ch in enumerate(characters)}\n",
        "  ix2char = {i:ch for i, ch in enumerate(characters)}\n",
        "  \n",
        "  return characters, char2ix, ix2char\n",
        "\n",
        "def initialize_case_embeddings(vocab=['upper','lower','title','numeric','other']):\n",
        "  case2ix = {case:i for i, case in enumerate(vocab)}\n",
        "  ix2case = {}\n",
        "  cases = []\n",
        "  for k,v in case2ix.items():\n",
        "    this_case = np.zeros(len(case2ix))\n",
        "    this_case[v] = 1\n",
        "    cases.append(this_case)\n",
        "    ix2case[v] = k\n",
        "  cases = np.array(cases)\n",
        "  \n",
        "  return cases, case2ix, ix2case\n",
        "\n",
        "  \n",
        "def initialize_labels(file_name):\n",
        "  data = pd.read_csv(os.path.join(data_src, file_name), sep=\"\\t\",  quoting=csv.QUOTE_NONE, header=None, skip_blank_lines=False, engine='python', names =['token', 'pos', 'tree', 'BIO'])\n",
        "  data.dropna(subset=['BIO'], inplace=True)\n",
        "  label_list = data.BIO.unique()\n",
        "  label2ix = {label:i for i, label in enumerate(label_list)}\n",
        "  ix2label = {i:label for i, label in enumerate(label_list)}\n",
        "  return label_list, label2ix, ix2label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgGc-vs3cWbg",
        "colab_type": "code",
        "outputId": "00a2b625-6dd9-429d-9d82-80b475f3be35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# load embeddings and format\n",
        "words, word_to_ix, ix_to_word = initialize_word_embeddings(embedding_file, use_cache=True)\n",
        "characters, char_to_ix, ix_to_char = initialize_character_embeddings()\n",
        "cases, case_to_ix, ix_to_case = initialize_case_embeddings()\n",
        "labels, label_to_ix, ix_to_label = initialize_labels(train_file)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attempting to load from cache\n",
            "Loaded successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9-ODReesbH4",
        "colab_type": "text"
      },
      "source": [
        "## Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxVpGgDwfGSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def checkPrior(blah):\n",
        "  if blah is None:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "  \n",
        "def phrase2char(w_vec):\n",
        "  '''\n",
        "  This function transforms a sequence of words in index format to a 2d array of character indexes\n",
        "  \n",
        "  w_vec - an iterable of word indexes\n",
        "  \n",
        "  returns np.ndarray of size (len(w_vec), WORD_WIDTH)\n",
        "  '''\n",
        "  phrase_vector = []\n",
        "  for w_ix in w_vec:\n",
        "    char_vector = []\n",
        "    if w_ix not in (word_to_ix[PAD_WORD],word_to_ix[UNK_WORD]):\n",
        "      for char in ix_to_word[w_ix]:\n",
        "        char_vector.append(get_char_ix(char))\n",
        "    phrase_vector.append(np.array(char_vector))\n",
        "  return pad_sequences(phrase_vector, value=char_to_ix[PAD_CHAR], maxlen=WORD_WIDTH, padding='post')\n",
        "\n",
        "def pad_truncate(x,width,pad_token):\n",
        "  if(len(x) > width):\n",
        "    print(f\"Truncating input: {[ix_to_word[ix] for ix in x]}\")\n",
        "    x = x[:256]\n",
        "  return np.pad(x,pad_width=(0,width-len(x)), mode='constant', constant_values=pad_token)\n",
        "\n",
        "def verbosity(str, verbose):\n",
        "  if verbose:\n",
        "    print(str)\n",
        "\n",
        "def preprocess_data(file_name, use_cache=True, debug=True):\n",
        "  '''\n",
        "  Prepares data for model.  It can be used for both training and test data.\n",
        "  \n",
        "  returns pd.DataFrame\n",
        "  '''\n",
        "  clean_name = os.path.join(cache_dir, file_name.replace(\".\", \"_\"))\n",
        "  loaded = False\n",
        "  phrase_vectors = None\n",
        "      \n",
        "  if use_cache and os.path.exists(clean_name+\"word.npy\"):\n",
        "    verbosity(\"Attempting to load from cache\", debug)\n",
        "    try:\n",
        "      word_vectors = np.load(clean_name+\"word.npy\", allow_pickle=True)\n",
        "      char_vectors = np.load(clean_name+\"char.npy\", allow_pickle=True)\n",
        "      case_vectors = np.load(clean_name+\"case.npy\", allow_pickle=True)\n",
        "      label_vectors = np.load(clean_name+\"label.npy\", allow_pickle=True)\n",
        "      phrase_vectors = [word_vectors, char_vectors, case_vectors, label_vectors]\n",
        "      loaded = True\n",
        "      verbosity(\"Loaded successfully\", debug)\n",
        "    except:\n",
        "      verbosity(\"Loading failed\",debug)\n",
        "      loaded = False\n",
        "  \n",
        "  if not loaded:\n",
        "    verbosity(f\"Loading raw data file to process labels: {file_name}\", debug)\n",
        "    checkpoint = time.time()  \n",
        "    data = pd.read_csv(os.path.join(data_src, file_name), sep=\"\\t\",  quoting=csv.QUOTE_NONE, header=None, skip_blank_lines=False, engine='python', names =['token', 'pos', 'tree', 'BIO'])\n",
        "    verbosity(f\"Parsed data loaded: {time.time()-checkpoint} s\", debug)\n",
        "\n",
        "    # see if prior row was a newline\n",
        "    data['prior'] = data.token.shift(1)\n",
        "    # drop empty rows\n",
        "    data = data.loc[~data.token.isnull()]\n",
        "    data.prior = data.prior.apply(checkPrior)\n",
        "    data['phrase'] = data.prior.cumsum()\n",
        "        \n",
        "    verbosity(\"Processing data into phrase vectors\", debug)\n",
        "    verbosity(\"Step 1: Translating to indexes\", debug)\n",
        "    checkpoint = time.time()\n",
        "    data['word_ix'] = data.token.apply(get_word_ix)\n",
        "    data['case_ix'] = data.token.apply(get_casing_ix)\n",
        "    data['label_ix'] = data.BIO.apply(lambda x: label_to_ix[x])\n",
        "    verbosity(f\"Step 1: Translated to indexes complete: {time.time()-checkpoint} s\", debug)\n",
        "\n",
        "    verbosity(\"Step 2: Creating phrase vectors\", debug)\n",
        "    verbosity(\"Step 2a: Aggregating phrases\", debug)\n",
        "    checkpoint = time.time()\n",
        "    phrase_vectors = data.groupby('phrase').agg({'word_ix': list, 'case_ix': list, 'label_ix': list})\n",
        "    verbosity(f\"Step 2a: {time.time()-checkpoint} s\", debug)\n",
        "    \n",
        "    verbosity(\"Step 2b: Padding word vectors\", debug)\n",
        "    checkpoint = time.time()\n",
        "    phrase_vectors['word_vector'] = phrase_vectors.word_ix.apply(lambda x: pad_truncate(x, SENTENCE_WIDTH, word_to_ix[PAD_WORD]))\n",
        "    verbosity(f\"Step 2b: {time.time()-checkpoint} s\", debug)\n",
        "    \n",
        "    verbosity(\"Step 2c: Creating and padding character vectors\", debug)\n",
        "    checkpoint = time.time()\n",
        "    phrase_vectors['char_vector'] = phrase_vectors.word_vector.apply(lambda x: phrase2char(x))\n",
        "    verbosity(f\"Step 2c: {time.time()-checkpoint} s\", debug)\n",
        "    \n",
        "    verbosity(f\"Step 2d: Padding case vectors\", debug)\n",
        "    checkpoint = time.time()\n",
        "    phrase_vectors['case_vector'] = phrase_vectors.case_ix.apply(lambda x: pad_truncate(x, SENTENCE_WIDTH, case_to_ix[CASE_PAD_SYMBOL]))\n",
        "    verbosity(f\"Step 2d: {time.time()-checkpoint}\", debug)\n",
        "    \n",
        "    verbosity(\"Step 2e: Padding label vectors\", debug)\n",
        "    checkpoint = time.time()\n",
        "    phrase_vectors['label_vector'] = phrase_vectors.label_ix.apply(lambda x: np.expand_dims(pad_truncate(x, SENTENCE_WIDTH, label_to_ix[LABEL_PAD_SYMBOL]), -1))\n",
        "    verbosity(f\"Step 2e: {time.time()-checkpoint} s\", debug)\n",
        "    \n",
        "    verbosity(\"Saving data to disk\", debug)\n",
        "    checkpoint = time.time()\n",
        "    phrase_vectors.drop(columns=['word_ix', 'case_ix', 'label_ix'], inplace=True)\n",
        "    phrase_vectors = phrase_vectors.to_numpy()\n",
        "    phrase_vectors = [np.stack(phrase_vectors[:,0]), np.stack(phrase_vectors[:,1]), np.stack(phrase_vectors[:,2]), np.stack(phrase_vectors[:,3])]\n",
        "    \n",
        "    # saving in multi parts because training data causes a memory error\n",
        "    np.save(clean_name+'word', phrase_vectors[0], allow_pickle=True)\n",
        "    np.save(clean_name+'char', phrase_vectors[1], allow_pickle=True)\n",
        "    np.save(clean_name+'case', phrase_vectors[2], allow_pickle=True)\n",
        "    np.save(clean_name+'label', phrase_vectors[3], allow_pickle=True)\n",
        "\n",
        "    verbosity(f\"Saved to disk: {time.time()-checkpoint} s\", debug)\n",
        "  \n",
        "  return phrase_vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_w_YjGLsgeQ",
        "colab_type": "code",
        "outputId": "5efcddc3-2208-49ef-f7fe-c4d2b4e4de38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "train_data = preprocess_data(train_file, use_cache=True)\n",
        "print(train_data[0].shape,train_data[1].shape,train_data[2].shape,train_data[3].shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attempting to load from cache\n",
            "Loaded successfully\n",
            "(115812, 256) (115812, 256, 52) (115812, 256) (115812, 256, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7drZlMCSsUB",
        "colab_type": "text"
      },
      "source": [
        "## Prepare development data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY98eRh_SrYx",
        "colab_type": "code",
        "outputId": "51b45a17-f0bf-4452-d382-87f82b0f0175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "dev_data = preprocess_data(dev_file, use_cache=True)\n",
        "print(dev_data[0].shape,dev_data[1].shape,dev_data[2].shape,dev_data[3].shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attempting to load from cache\n",
            "Loaded successfully\n",
            "(15680, 256) (15680, 256, 52) (15680, 256) (15680, 256, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TtSmxhhSvNK",
        "colab_type": "text"
      },
      "source": [
        "## Prepare test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVvhFxt0S4cf",
        "colab_type": "code",
        "outputId": "7509c89f-0d30-4023-ff31-cc479b358487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "test_data = preprocess_data(test_file, use_cache=True)\n",
        "print(test_data[0].shape,test_data[1].shape,test_data[2].shape,test_data[3].shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attempting to load from cache\n",
            "Loaded successfully\n",
            "(12217, 256) (12217, 256, 52) (12217, 256) (12217, 256, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9EoodmDDgbI",
        "colab_type": "text"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg8mZc-1DlD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/mxhofer/Named-Entity-Recognition-BidirectionalLSTM-CNN-CoNLL/blob/master/nn_CoNLL.ipynb\n",
        "\n",
        "def buildModel(labels, wordEmbeddings, caseEmbeddings, characterEmbeddings=None):\n",
        "  \"\"\"Model layers\"\"\"\n",
        "  \n",
        "  # character input\n",
        "  character_input = Input(shape=(None, config.word_length,), name=\"Character_input\")\n",
        "  embed_char_out = TimeDistributed(\n",
        "      Embedding(config.char_vocab, config.char_embedding_dim, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name=\"Character_embedding\")(\n",
        "      character_input)\n",
        "\n",
        "  dropout = Dropout(config.dropout)(embed_char_out)\n",
        "\n",
        "  # CNN\n",
        "  conv1d_out = TimeDistributed(Conv1D(kernel_size=config.conv_size, filters=config.conv_filters, padding='same', activation='tanh', strides=config.conv_stride), name=\"Convolution\")(dropout)\n",
        "  maxpool_out = TimeDistributed(MaxPooling1D(config.conv_window), name=\"Maxpool\")(conv1d_out)\n",
        "  char = TimeDistributed(Flatten(), name=\"Flatten\")(maxpool_out)\n",
        "  char = Dropout(config.dropout)(char)\n",
        "\n",
        "  # word-level input\n",
        "  words_input = Input(shape=(None,), dtype='int32', name='words_input')\n",
        "  words = Embedding(input_dim=wordEmbeddings.shape[0], output_dim=wordEmbeddings.shape[1], weights=[wordEmbeddings],\n",
        "                    trainable=False)(words_input)\n",
        "\n",
        "  # case-info input\n",
        "  casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
        "  casing = Embedding(input_dim=caseEmbeddings.shape[0], output_dim=caseEmbeddings.shape[1], weights=[caseEmbeddings],\n",
        "                     trainable=False)(casing_input)\n",
        "  \n",
        "  # concat & BLSTM\n",
        "  output = concatenate([words, casing, char])\n",
        "  output = Bidirectional(LSTM(config.lstm_state_size, \n",
        "                              return_sequences=True, \n",
        "                              dropout=config.dropout,                        # on input to each LSTM block\n",
        "                              recurrent_dropout=config.recurrent_dropout     # on recurrent input signal\n",
        "                             ), name=\"BLSTM\")(output)\n",
        "  output = TimeDistributed(Dense(len(labels), activation='softmax'),name=\"Softmax_layer\")(output)\n",
        "\n",
        "  # set up model\n",
        "  model = Model(inputs=[words_input, character_input, casing_input], outputs=[output])\n",
        "\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=Nadam())\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxs36GeTkPVh",
        "colab_type": "code",
        "outputId": "ff0cca37-67b6-4867-9bda-1da98091f9d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "myModel = None\n",
        "if os.path.exists(model_load_path):\n",
        "  print(\"Attempting to load model\")\n",
        "  myModel = load_model(model_load_path)\n",
        "  print(\"Model loaded successfully\")\n",
        "  myModel.summary()\n",
        "  myModel.fit([train_data[0][:config.training_size],train_data[1][:config.training_size],train_data[2][:config.training_size]], train_data[3][:config.training_size],\n",
        "              validation_data = (dev_data[:3], dev_data[3]),\n",
        "              epochs=config.epochs,\n",
        "              initial_epoch=INITIAL_EPOCH,\n",
        "              batch_size=config.batch_size,\n",
        "              callbacks=[EarlyStopping(min_delta=0), ModelCheckpoint(model_path)])\n",
        "  myModel.save(os.path.join(wandb.run.dir, \"model.h5\"))\n",
        "else:\n",
        "  print(\"Building model\")\n",
        "  myModel = buildModel(labels, words, cases)\n",
        "  myModel.summary()\n",
        "  myModel.fit([train_data[0][:config.training_size],train_data[1][:config.training_size],train_data[2][:config.training_size]], train_data[3][:config.training_size],\n",
        "              validation_data = (dev_data[:3], dev_data[3]),\n",
        "              epochs=config.epochs,\n",
        "              initial_epoch=0,\n",
        "              batch_size=config.batch_size,\n",
        "              callbacks=[EarlyStopping(min_delta=0), ModelCheckpoint(model_path)])\n",
        "  myModel.save(os.path.join(wandb.run.dir, \"model.h5\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0727 17:19:33.942376 139775883548544 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0727 17:19:33.973322 139775883548544 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0727 17:19:33.979979 139775883548544 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0727 17:19:34.013591 139775883548544 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0727 17:19:34.024033 139775883548544 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0727 17:19:34.097714 139775883548544 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Building model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0727 17:19:34.162523 139775883548544 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0727 17:19:37.512042 139775883548544 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0727 17:19:37.675576 139775883548544 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Character_input (InputLayer)    (None, None, 52)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Character_embedding (TimeDistri (None, None, 52, 30) 3000        Character_input[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, None, 52, 30) 0           Character_embedding[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "Convolution (TimeDistributed)   (None, None, 52, 30) 2730        dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Maxpool (TimeDistributed)       (None, None, 1, 30)  0           Convolution[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "words_input (InputLayer)        (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "casing_input (InputLayer)       (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Flatten (TimeDistributed)       (None, None, 30)     0           Maxpool[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, None, 50)     19999950    words_input[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, None, 5)      25          casing_input[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, None, 30)     0           Flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, None, 85)     0           embedding_2[0][0]                \n",
            "                                                                 embedding_3[0][0]                \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "BLSTM (Bidirectional)           (None, None, 400)    457600      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Softmax_layer (TimeDistributed) (None, None, 37)     14837       BLSTM[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 20,478,142\n",
            "Trainable params: 478,167\n",
            "Non-trainable params: 19,999,975\n",
            "__________________________________________________________________________________________________\n",
            "Train on 115812 samples, validate on 15680 samples\n",
            "Epoch 1/20\n",
            "115812/115812 [==============================] - 700s 6ms/step - loss: 0.0655 - val_loss: 0.0196\n",
            "Epoch 2/20\n",
            "115812/115812 [==============================] - 694s 6ms/step - loss: 0.0158 - val_loss: 0.0138\n",
            "Epoch 3/20\n",
            "115812/115812 [==============================] - 695s 6ms/step - loss: 0.0128 - val_loss: 0.0121\n",
            "Epoch 4/20\n",
            "115812/115812 [==============================] - 697s 6ms/step - loss: 0.0112 - val_loss: 0.0107\n",
            "Epoch 5/20\n",
            "115812/115812 [==============================] - 697s 6ms/step - loss: 0.0103 - val_loss: 0.0099\n",
            "Epoch 6/20\n",
            " 18400/115812 [===>..........................] - ETA: 9:28 - loss: 0.0100"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6CpwuNAYgv4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_metrics(model, data, save=True):\n",
        "  predictions = model.predict(data[:3])\n",
        "  y = data[3].reshape(data[3].shape[0],data[3].shape[1])\n",
        "  \n",
        "  pf = np.argmax(predictions, axis=2).flatten()\n",
        "  af = data[3].flatten()\n",
        "  \n",
        "  metrics = []\n",
        "  metrics = pd.DataFrame(columns=['Label', 'Support', 'Precision', 'Recall', \"F1\"])\n",
        "  for i, label in enumerate(labels):\n",
        "    support = np.where(af == i)\n",
        "    tp = np.sum(pf[support] == af[support])\n",
        "\n",
        "    precision = None\n",
        "    if pf[np.where(pf == i)].shape[0] == 0:\n",
        "      precision = 0.0\n",
        "    else:\n",
        "      precision = tp/pf[np.where(pf == i)].shape[0]\n",
        "      \n",
        "    recall = tp/af[support].shape[0]\n",
        "    \n",
        "    f1 = None\n",
        "    if precision + recall == 0:\n",
        "      f1 = 0\n",
        "    else:\n",
        "      f1 = 2*precision*recall/(precision+recall)\n",
        "\n",
        "    metrics = metrics.append({'Label': ix_to_label[i], 'Support':af[support].shape[0], 'Precision': precision, 'Recall':recall, 'F1':f1}, ignore_index=True)\n",
        "  \n",
        "  metrics = metrics.append({'Label': 'micro',\n",
        "                  'Support': metrics.Support.sum(),\n",
        "                  'Precision': (metrics.Precision*metrics.Support/pf.shape[0]).sum(),\n",
        "                  'Recall': (metrics.Recall*metrics.Support/pf.shape[0]).sum(),\n",
        "                  'F1': (metrics.F1*metrics.Support/pf.shape[0]).sum()\n",
        "                           },\n",
        "                 ignore_index=True)\n",
        "  metrics = metrics.set_index('Label')\n",
        "  if save:\n",
        "    metrics.to_csv(model_results_path)\n",
        "  display(metrics)\n",
        "  return predictions\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcwoqTbN5WQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = get_metrics(myModel, test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}