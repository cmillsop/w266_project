{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_BILOU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8oOGLgN_KEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from time import time\n",
        "import io\n",
        "import re\n",
        "import csv\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import model_from_json\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support,classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.python.keras.layers import Lambda\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.layers import Dense, TimeDistributed\n",
        "from tensorflow.python.tools import inspect_checkpoint as chkp\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92lSZj9R_VU3",
        "colab_type": "code",
        "outputId": "41337cc8-a89e-4f5b-8687-dc494832acfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aim4djhc_Vyn",
        "colab_type": "code",
        "outputId": "7001d422-2f50-4582-ce32-17998a92a196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "__dir__ = '/content/drive/My Drive/Colab Notebooks/'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sOUmrkM_V62",
        "colab_type": "code",
        "outputId": "f3413303-6f1c-48f6-af5e-399a939145e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6V4JhZ6_V92",
        "colab_type": "code",
        "outputId": "3c3a3281-094a-4585-90f9-9bc746c592a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0803 23:34:16.901755 140659012458368 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI1taT00_WBO",
        "colab_type": "code",
        "outputId": "524f75bc-cd8e-489c-dacc-c72bb689c4db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "OUTPUT_DIR = '/content/drive/My Drive/Colab Notebooks/Model_Output/'\n",
        "tf.io.gfile.makedirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: /content/drive/My Drive/Colab Notebooks/Model_Output/ *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HZKxETI_WEH",
        "colab_type": "code",
        "outputId": "6aa43f31-c27c-48df-c539-11f8b0612638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1'\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info =  bert_module(signature='tokenization_info', as_dict=True)\n",
        "#     print(tokenization_info)\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "      vocab_file = sess.run(tokenization_info['vocab_file'])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(vocab_file=vocab_file)\n",
        "\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0803 23:34:30.036031 140659012458368 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF4FCA5D_stR",
        "colab_type": "code",
        "outputId": "b3ba5a58-ec3f-43f6-f8b5-2e5f162deb75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'here',\n",
              " \"'\",\n",
              " 's',\n",
              " 'an',\n",
              " 'example',\n",
              " 'of',\n",
              " 'using',\n",
              " 'the',\n",
              " 'be',\n",
              " '##rt',\n",
              " 'token',\n",
              " '##izer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdJWYaf-_wJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_df(data_type):\n",
        "  data_type_dict = {'train' : '/content/drive/My Drive/Colab Notebooks/data/onto.train.ner_bilou.csv',\n",
        "                   'dev' : '/content/drive/My Drive/Colab Notebooks/data/onto.development.ner_bilou.csv',\n",
        "                   'test' : '/content/drive/My Drive/Colab Notebooks/data/onto.test.ner_bilou.csv',\n",
        "                   'bios': '/content/drive/My Drive/Colab Notebooks/data/bios-tagged-final-flat_bilou_no_title.csv'}\n",
        "\n",
        "  df = pd.read_csv(data_type_dict[data_type])\n",
        "  \n",
        "  df['x'] = df['x'].apply(ast.literal_eval)\n",
        "  X = df['x'].apply(' '.join)\n",
        "  y = df['y'].apply(ast.literal_eval)\n",
        "   \n",
        "  return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sDUAxN6_wZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, y_train = get_df('train')\n",
        "X_dev, y_dev = get_df('dev')\n",
        "X_test, y_test = get_df('test')\n",
        "X_bios, y_bios = get_df('bios')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp_RVZUTFpxK",
        "colab_type": "code",
        "outputId": "cd0d003f-6b8b-4f80-b83d-b52dcde02df4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What kind of memory ?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwhLcjTG_whh",
        "colab_type": "code",
        "outputId": "56e833dc-2725-4538-eab5-621c09bda5a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "label_list = list(set([label for target_list in y_train for label in target_list])) + ['[CLS]', '[SEP]']\n",
        "# print(label_list)\n",
        "label_map = {label : i for i, label in enumerate(label_list,1)}\n",
        "print(label_map)\n",
        "print(f'Length of labels - {len(label_list)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'U-PRODUCT': 1, 'I-QUANTITY': 2, 'U-PERCENT': 3, 'U-WORK_OF_ART': 4, 'L-PRODUCT': 5, 'B-ORG': 6, 'I-ORG': 7, 'U-ORG': 8, 'L-WORK_OF_ART': 9, 'B-PERCENT': 10, 'B-CARDINAL': 11, 'L-ORDINAL': 12, 'B-NORP': 13, 'B-GPE': 14, 'I-MONEY': 15, 'U-DATE': 16, 'L-LAW': 17, 'B-WORK_OF_ART': 18, 'L-NORP': 19, 'L-QUANTITY': 20, 'B-LANGUAGE': 21, 'L-LOC': 22, 'L-PERCENT': 23, 'B-PERSON': 24, 'U-ORDINAL': 25, 'L-PERSON': 26, 'U-MONEY': 27, 'B-LOC': 28, 'U-GPE': 29, 'I-DATE': 30, 'U-LANGUAGE': 31, 'L-DATE': 32, 'U-QUANTITY': 33, 'I-FAC': 34, 'B-TIME': 35, 'B-DATE': 36, 'I-PERSON': 37, 'I-ORDINAL': 38, 'L-CARDINAL': 39, 'I-CARDINAL': 40, 'I-WORK_OF_ART': 41, 'B-ORDINAL': 42, 'I-NORP': 43, 'U-TIME': 44, 'U-NORP': 45, 'L-EVENT': 46, 'B-QUANTITY': 47, 'I-LOC': 48, 'L-MONEY': 49, 'B-LAW': 50, 'L-FAC': 51, 'I-GPE': 52, 'U-PERSON': 53, 'U-CARDINAL': 54, 'L-ORG': 55, 'B-PRODUCT': 56, 'L-LANGUAGE': 57, 'B-MONEY': 58, 'B-FAC': 59, 'I-PERCENT': 60, 'U-FAC': 61, 'I-TIME': 62, 'U-LAW': 63, 'I-PRODUCT': 64, 'B-EVENT': 65, 'I-EVENT': 66, 'I-LAW': 67, 'L-GPE': 68, 'I-LANGUAGE': 69, 'L-TIME': 70, 'U-EVENT': 71, 'O': 72, 'U-LOC': 73, '[CLS]': 74, '[SEP]': 75}\n",
            "Length of labels - 75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyVnX0yG_wmQ",
        "colab_type": "code",
        "outputId": "b322913b-d5d3-4309-ae31-5dad0eee79bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "label_map = {label : i for i, label in enumerate(label_list,1)}\n",
        "print(label_map)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'U-PRODUCT': 1, 'I-QUANTITY': 2, 'U-PERCENT': 3, 'U-WORK_OF_ART': 4, 'L-PRODUCT': 5, 'B-ORG': 6, 'I-ORG': 7, 'U-ORG': 8, 'L-WORK_OF_ART': 9, 'B-PERCENT': 10, 'B-CARDINAL': 11, 'L-ORDINAL': 12, 'B-NORP': 13, 'B-GPE': 14, 'I-MONEY': 15, 'U-DATE': 16, 'L-LAW': 17, 'B-WORK_OF_ART': 18, 'L-NORP': 19, 'L-QUANTITY': 20, 'B-LANGUAGE': 21, 'L-LOC': 22, 'L-PERCENT': 23, 'B-PERSON': 24, 'U-ORDINAL': 25, 'L-PERSON': 26, 'U-MONEY': 27, 'B-LOC': 28, 'U-GPE': 29, 'I-DATE': 30, 'U-LANGUAGE': 31, 'L-DATE': 32, 'U-QUANTITY': 33, 'I-FAC': 34, 'B-TIME': 35, 'B-DATE': 36, 'I-PERSON': 37, 'I-ORDINAL': 38, 'L-CARDINAL': 39, 'I-CARDINAL': 40, 'I-WORK_OF_ART': 41, 'B-ORDINAL': 42, 'I-NORP': 43, 'U-TIME': 44, 'U-NORP': 45, 'L-EVENT': 46, 'B-QUANTITY': 47, 'I-LOC': 48, 'L-MONEY': 49, 'B-LAW': 50, 'L-FAC': 51, 'I-GPE': 52, 'U-PERSON': 53, 'U-CARDINAL': 54, 'L-ORG': 55, 'B-PRODUCT': 56, 'L-LANGUAGE': 57, 'B-MONEY': 58, 'B-FAC': 59, 'I-PERCENT': 60, 'U-FAC': 61, 'I-TIME': 62, 'U-LAW': 63, 'I-PRODUCT': 64, 'B-EVENT': 65, 'I-EVENT': 66, 'I-LAW': 67, 'L-GPE': 68, 'I-LANGUAGE': 69, 'L-TIME': 70, 'U-EVENT': 71, 'O': 72, 'U-LOC': 73, '[CLS]': 74, '[SEP]': 75}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQtjumWZ_wrR",
        "colab_type": "code",
        "outputId": "c24955ef-110d-4a1c-abc8-20f9c4fbb6ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(X_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What kind of memory ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZzOtFbcEFQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_to_df(X, y):\n",
        "\n",
        "  df = pd.DataFrame([X,y]).T\n",
        "  df.columns = ['X', 'y']\n",
        "#   print (type(df.loc[0, 'X']))\n",
        "#   print (type(df.loc[0, 'y']))\n",
        "#   df['y'] = df['y'].str.strip('[]')\n",
        "#   print(df.head())\n",
        "  return df\n",
        "\n",
        "train = conv_to_df(X_train, y_train)\n",
        "dev = conv_to_df(X_dev, y_dev)\n",
        "test = conv_to_df(X_test, y_test)\n",
        "bios = conv_to_df(X_bios, y_bios)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujlS26qTykVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukn-saAkEJSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id, valid_ids=None, label_mask=None):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "        self.valid_ids = valid_ids\n",
        "        self.label_mask = label_mask\n",
        "\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    label_map = {label : i for i, label in enumerate(label_list,1)}\n",
        "    \n",
        "    features = []\n",
        "    for (ex_index,example) in enumerate(examples):\n",
        "        textlist = example.text_a.split(' ')\n",
        "        labellist = example.label\n",
        "        if len(textlist) != len(labellist):\n",
        "          continue\n",
        "        else:\n",
        "          tokens = []\n",
        "          labels = []\n",
        "          valid = []\n",
        "          label_mask = []\n",
        "          for i, word in enumerate(textlist): \n",
        "              token = tokenizer.tokenize(word)\n",
        "              tokens.extend(token)\n",
        "\n",
        "              label_1 = labellist[i]\n",
        "              for m in range(len(token)):\n",
        "                  if m == 0:\n",
        "                      labels.append(label_1)\n",
        "                      valid.append(1)\n",
        "                      label_mask.append(1)\n",
        "                  else:\n",
        "                      valid.append(0)\n",
        "          if len(tokens) >= max_seq_length - 1:\n",
        "              tokens = tokens[0:(max_seq_length - 2)]\n",
        "              labels = labels[0:(max_seq_length - 2)]\n",
        "              valid = valid[0:(max_seq_length - 2)]\n",
        "              label_mask = label_mask[0:(max_seq_length - 2)]\n",
        "          ntokens = []\n",
        "          segment_ids = []\n",
        "          label_ids = []\n",
        "          ntokens.append(\"[CLS]\")\n",
        "          segment_ids.append(0)\n",
        "          valid.insert(0,1)\n",
        "          label_mask.insert(0,1)\n",
        "          label_ids.append(label_map[\"[CLS]\"])\n",
        "          for i, token in enumerate(tokens):\n",
        "              ntokens.append(token)\n",
        "              segment_ids.append(0)\n",
        "              if len(labels) > i:\n",
        "                  label_ids.append(label_map[labels[i]])\n",
        "          ntokens.append(\"[SEP]\")\n",
        "          segment_ids.append(0)\n",
        "          valid.append(1)\n",
        "          label_mask.append(1)\n",
        "          label_ids.append(label_map[\"[SEP]\"])\n",
        "          input_ids = tokenizer.convert_tokens_to_ids(ntokens)\n",
        "          input_mask = [1] * len(input_ids)\n",
        "          label_mask = [1] * len(label_ids)\n",
        "          while len(input_ids) < max_seq_length:\n",
        "              input_ids.append(0)\n",
        "              input_mask.append(0)\n",
        "              segment_ids.append(0)\n",
        "              label_ids.append(0)\n",
        "              valid.append(1)\n",
        "              label_mask.append(0)\n",
        "          while len(label_ids) < max_seq_length:\n",
        "              label_ids.append(0)\n",
        "              label_mask.append(0)\n",
        "          assert len(input_ids) == max_seq_length\n",
        "          assert len(input_mask) == max_seq_length\n",
        "          assert len(segment_ids) == max_seq_length\n",
        "          assert len(label_ids) == max_seq_length\n",
        "          assert len(valid) == max_seq_length\n",
        "          assert len(label_mask) == max_seq_length\n",
        "\n",
        "          if ex_index < 5:\n",
        "              logger.info(\"*** Example ***\")\n",
        "              logger.info(\"guid: %s\" % (example.guid))\n",
        "              logger.info(\"tokens: %s\" % \" \".join(\n",
        "                      [str(x) for x in tokens]))\n",
        "              logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "              logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "              logger.info(\n",
        "                      \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "#               logger.info(\"label: %s (id = %d)\" % (example.label, label_ids))\n",
        "\n",
        "          features.append(\n",
        "                  InputFeatures(input_ids=input_ids,\n",
        "                                input_mask=input_mask,\n",
        "                                segment_ids=segment_ids,\n",
        "                                label_id=label_ids,\n",
        "                                valid_ids=valid,\n",
        "                                label_mask=label_mask))\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_P2lid0EO5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x['X'], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x['y']), axis = 1)\n",
        "\n",
        "dev_InputExamples = dev.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x['X'], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x['y']), axis = 1)\n",
        "\n",
        "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x['X'], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x['y']), axis = 1)\n",
        "\n",
        "bios_InputExamples = bios.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x['X'], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x['y']), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XzHN3l-ESak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQ_LENGTH = 56\n",
        "max_length = MAX_SEQ_LENGTH"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEcgDfwKEbBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zn8yKTzEeOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_features = convert_examples_to_features(dev_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3SkP7RgGuP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_features = convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKsonXKlN4Sy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bios_features = convert_examples_to_features(bios_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT8JZVzLGz97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainSentence_ids = []\n",
        "trainMasks = []\n",
        "trainSequence_ids = []\n",
        "\n",
        "devSentence_ids = []\n",
        "devMasks = []\n",
        "devSequence_ids = []\n",
        "\n",
        "testSentence_ids = []\n",
        "testMasks = []\n",
        "testSequence_ids = []\n",
        "\n",
        "biosSentence_ids = []\n",
        "biosMasks = []\n",
        "biosSequence_ids = []\n",
        "\n",
        "nerLabels_train =[]\n",
        "nerLabels_dev = []\n",
        "nerLabels_test = []\n",
        "nerLabels_bios = []\n",
        "\n",
        "bert_inputs_train = np.array([[f.input_ids for f in train_features], [f.input_mask for f in train_features],[f.segment_ids for f in train_features]])\n",
        "labels_train = [f.label_id for f in train_features]\n",
        "num_sentences_train = len([f.input_ids for f in train_features])\n",
        "# print(num_sentences)\n",
        "for example in range(num_sentences_train):\n",
        "    trainSentence_ids.append(bert_inputs_train[0][example])\n",
        "    trainMasks.append(bert_inputs_train[1][example])\n",
        "    trainSequence_ids.append(bert_inputs_train[2][example])\n",
        "    nerLabels_train.append(labels_train[example])\n",
        "\n",
        "    \n",
        "# labels_train = [to_categorical(f.label_id) for f in train_features]\n",
        "\n",
        "\n",
        "bert_inputs_dev = [[f.input_ids for f in dev_features], [f.input_mask for f in dev_features],[f.segment_ids for f in dev_features]]\n",
        "labels_dev = [f.label_id for f in dev_features]\n",
        "num_sentences_dev = len([f.input_ids for f in dev_features])\n",
        "for example in range(num_sentences_dev):\n",
        "    devSentence_ids.append(bert_inputs_dev[0][example])\n",
        "    devMasks.append(bert_inputs_dev[1][example])\n",
        "    devSequence_ids.append(bert_inputs_dev[2][example])\n",
        "    nerLabels_dev.append(labels_dev[example])\n",
        "\n",
        "    \n",
        "bert_inputs_test = np.array([[f.input_ids for f in test_features], [f.input_mask for f in test_features],[f.segment_ids for f in test_features]])\n",
        "labels_test = [f.label_id for f in test_features]\n",
        "num_sentences_test = len([f.input_ids for f in test_features])\n",
        "# print(num_sentences)\n",
        "for example in range(num_sentences_test):\n",
        "    testSentence_ids.append(bert_inputs_test[0][example])\n",
        "    testMasks.append(bert_inputs_test[1][example])\n",
        "    testSequence_ids.append(bert_inputs_test[2][example])\n",
        "    nerLabels_test.append(labels_test[example])    \n",
        "    \n",
        "    \n",
        "bert_inputs_bios = np.array([[f.input_ids for f in bios_features], [f.input_mask for f in bios_features],[f.segment_ids for f in bios_features]])\n",
        "labels_bios = [f.label_id for f in bios_features]\n",
        "num_sentences_bios = len([f.input_ids for f in bios_features])\n",
        "# print(num_sentences)\n",
        "for example in range(num_sentences_bios):\n",
        "    biosSentence_ids.append(bert_inputs_bios[0][example])\n",
        "    biosMasks.append(bert_inputs_bios[1][example])\n",
        "    biosSequence_ids.append(bert_inputs_bios[2][example])\n",
        "    nerLabels_bios.append(labels_bios[example])    \n",
        "    \n",
        "X_train = np.array([trainSentence_ids,trainMasks,trainSequence_ids])\n",
        "X_dev = np.array([devSentence_ids,devMasks,devSequence_ids])\n",
        "X_test = np.array([testSentence_ids,testMasks,testSequence_ids])\n",
        "X_bios = np.array([biosSentence_ids,biosMasks,biosSequence_ids])\n",
        "\n",
        "nerLabels_train = np.array(nerLabels_train)\n",
        "nerLabels_dev = np.array(nerLabels_dev)\n",
        "nerLabels_test = np.array(nerLabels_test)\n",
        "nerLabels_bios = np.array(nerLabels_bios)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOUjowG4G3SL",
        "colab_type": "code",
        "outputId": "9d066dce-cba1-436c-9d84-234cf66468c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_bios.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 708, 56)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU90He2_G-uV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k_start = 0\n",
        "k_end = -1\n",
        "\n",
        "if k_end == -1:\n",
        "    k_end_train = X_train[0].shape[0]\n",
        "    k_end_dev = X_dev[0].shape[0]\n",
        "    k_end_test = X_test[0].shape[0]\n",
        "    k_end_bios = X_bios[0].shape[0]\n",
        "else:\n",
        "    k_end_train = k_end_dev = k_end\n",
        "    \n",
        "\n",
        "\n",
        "bert_inputs_train_k = [X_train[0][k_start:k_end_train], X_train[1][k_start:k_end_train], \n",
        "                       X_train[2][k_start:k_end_train]]\n",
        "bert_inputs_dev_k = [X_dev[0][k_start:k_end_dev], X_dev[1][k_start:k_end_dev], \n",
        "                      X_dev[2][k_start:k_end_dev]]\n",
        "bert_inputs_test_k = [X_test[0][k_start:k_end_test], X_test[1][k_start:k_end_test], \n",
        "                      X_test[2][k_start:k_end_test]]\n",
        "bert_inputs_bios_k = [X_bios[0][k_start:k_end_test], X_bios[1][k_start:k_end_test], \n",
        "                      X_bios[2][k_start:k_end_test]]\n",
        "\n",
        "labels_train_k = nerLabels_train[k_start:k_end_train]\n",
        "labels_dev_k = nerLabels_dev[k_start:k_end_dev]\n",
        "labels_test_k = nerLabels_test[k_start:k_end_test]\n",
        "labels_bios_k = nerLabels_bios[k_start:k_end_bios]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6utXB5UHG2B",
        "colab_type": "code",
        "outputId": "6eaecf1f-d65c-46e8-db54-33033afb0505",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "for item in bert_inputs_train_k:\n",
        "  print(item.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(115812, 56)\n",
            "(115812, 56)\n",
            "(115812, 56)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipr8a16mHKeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2VfGP4WHPPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Create BERT layer, following https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b\n",
        "    init:  initialize layer. Specify various parameters regarding output types and dimensions. Very important is\n",
        "           to set the number of trainable layers.\n",
        "    build: build the layer based on parameters\n",
        "    call:  call the BERT layer within a model\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        n_fine_tune_layers=10,\n",
        "        pooling=\"sequence\",\n",
        "        bert_url=\"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.n_fine_tune_layers = n_fine_tune_layers\n",
        "        self.trainable = True\n",
        "        self.output_size = 768\n",
        "        self.pooling = pooling\n",
        "        self.bert_url = bert_url\n",
        "\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bert = hub.Module(\n",
        "            self.bert_url, trainable=self.trainable, name=f\"{self.name}_module\"\n",
        "        )\n",
        "\n",
        "        # Remove unused layers\n",
        "        trainable_vars = self.bert.variables\n",
        "        trainable_vars = [\n",
        "                var\n",
        "                for var in trainable_vars\n",
        "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
        "            ]\n",
        "        trainable_layers = []\n",
        "\n",
        "\n",
        "        # Select how many layers to fine tune\n",
        "        for i in range(self.n_fine_tune_layers):\n",
        "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
        "\n",
        "        # Update trainable vars to contain only the specified layers\n",
        "        trainable_vars = [\n",
        "            var\n",
        "            for var in trainable_vars\n",
        "            if any([l in var.name for l in trainable_layers])\n",
        "        ]\n",
        "\n",
        "        # Add to trainable weights\n",
        "        for var in trainable_vars:\n",
        "            self._trainable_weights.append(var)\n",
        "\n",
        "        for var in self.bert.variables:\n",
        "            if var not in self._trainable_weights:\n",
        "                self._non_trainable_weights.append(var)\n",
        "\n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        input_ids, input_mask, segment_ids = inputs\n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
        "                \"sequence_output\"\n",
        "            ]\n",
        "\n",
        "        mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqeMFVnjHT45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def variable_summaries(var):\n",
        "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
        "    with tf.name_scope('summaries'):\n",
        "        mean = tf.reduce_mean(var)\n",
        "        tf.summary.scalar('mean', mean)\n",
        "        with tf.name_scope('stddev'):\n",
        "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "        tf.summary.scalar('stddev', stddev)\n",
        "        tf.summary.scalar('max', tf.reduce_max(var))\n",
        "        tf.summary.scalar('min', tf.reduce_min(var))\n",
        "        tf.summary.histogram('histogram', var)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbQzjzjsHUp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    calculate loss function explicitly, filtering out 'extra inserted labels'\n",
        "    \n",
        "    y_true: Shape: (batch x (max_length + 1) )\n",
        "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
        "    \n",
        "    returns:  cost\n",
        "    \"\"\"\n",
        "\n",
        "    #get labels and predictions\n",
        "    \n",
        "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int32)),[-1])\n",
        "#     print(y_label.eval())\n",
        "    mask = (y_label < 74)   # This mask is used to remove all tokens that do not correspond to the original base text.\n",
        "\n",
        "    y_label_masked = tf.boolean_mask(y_label, mask)  # mask the labels\n",
        "    \n",
        "    y_flat_pred = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float32)),[-1, 75])\n",
        "#     print(y_flat_pred.eval())\n",
        "    y_flat_pred_masked = tf.boolean_mask(y_flat_pred, mask) # mask the predictions\n",
        "    \n",
        "    return tf.reduce_mean(sparse_categorical_crossentropy(y_label_masked, y_flat_pred_masked,from_logits=False ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npUpf9xSHU0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_acc_orig_tokens(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    calculate loss dfunction filtering out also the newly inserted labels\n",
        "    \n",
        "    y_true: Shape: (batch x (max_length) )\n",
        "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
        "    \n",
        "    returns: accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    #get labels and predictions\n",
        "    \n",
        "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
        "    \n",
        "    mask = (y_label < 74)\n",
        "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
        "    \n",
        "    y_predicted = tf.math.argmax(input = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
        "                                                    [-1, 75]), axis=1)\n",
        "    \n",
        "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
        "\n",
        "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omacSjmKHU8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_acc_orig_non_other_tokens(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    calculate loss dfunction explicitly filtering out also the 'Other'- labels\n",
        "    \n",
        "    y_true: Shape: (batch x (max_length) )\n",
        "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
        "    \n",
        "    returns: accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    #get labels and predictions\n",
        "    \n",
        "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
        "    \n",
        "    mask = (y_label < 74)\n",
        "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
        "    \n",
        "    y_predicted = tf.math.argmax(input = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
        "                                                    [-1, 75]), axis=1)\n",
        "    \n",
        "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
        "\n",
        "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vs5qWDDrHVFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adam_customized = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.91, beta_2=0.999, epsilon=None, decay=0.1, amsgrad=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c55OM6UaIAR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def ner_model(max_input_length, train_layers, optimizer):\n",
        "    \"\"\"\n",
        "    Implementation of NER model\n",
        "    \n",
        "    variables:\n",
        "        max_input_length: number of tokens (max_length + 1)\n",
        "        train_layers: number of layers to be retrained\n",
        "        optimizer: optimizer to be used\n",
        "    \n",
        "    returns: model\n",
        "    \"\"\"\n",
        "    \n",
        "    in_id = tf.keras.layers.Input(shape=(max_length,), name=\"input_ids\")\n",
        "    in_mask = tf.keras.layers.Input(shape=(max_length,), name=\"input_masks\")\n",
        "    in_segment = tf.keras.layers.Input(shape=(max_length), name=\"segment_ids\")\n",
        "    \n",
        "    \n",
        "    bert_inputs = [in_id, in_mask, in_segment]\n",
        "    \n",
        "    bert_sequence = BertLayer(n_fine_tune_layers=train_layers)(bert_inputs)\n",
        "    \n",
        "    print(bert_sequence)\n",
        "    \n",
        "    dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(bert_sequence)\n",
        "    \n",
        "#     dense = tf.keras.layers.Dropout(rate=0.2)(dense)\n",
        "    \n",
        "    pred = tf.keras.layers.Dense(75, activation='softmax', name='ner')(dense)\n",
        "     \n",
        "    print('pred: ', pred)\n",
        "    \n",
        "    ## Prepare for multipe loss functions, although not used here\n",
        "    \n",
        "    losses = {\n",
        "        \"ner\": custom_loss,\n",
        "        }\n",
        "    lossWeights = {\"ner\": 1.0\n",
        "                  }\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
        "\n",
        "#     model.compile(loss=losses, optimizer=optimizer, metrics=[custom_acc_orig_tokens, \n",
        "#                                                           custom_acc_orig_non_other_tokens])\n",
        "    \n",
        "    model.compile(loss=custom_loss, optimizer=optimizer, metrics=[custom_acc_orig_tokens, \n",
        "                                                          custom_acc_orig_non_other_tokens])\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def initialize_vars(sess):\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    K.set_session(sess)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl3YqINjIIsl",
        "colab_type": "code",
        "outputId": "3da67a3e-fa27-4e98-d9e9-3f91b4755304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "model = ner_model(max_length , train_layers=10, optimizer = adam_customized)\n",
        "\n",
        "# Instantiate variables\n",
        "initialize_vars(sess)\n",
        "\n",
        "\n",
        "\n",
        "# tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
        "\n",
        "model.fit(\n",
        "    bert_inputs_train_k, \n",
        "    {\"ner\":labels_train_k},\n",
        "    validation_data=(bert_inputs_dev_k, {\"ner\":labels_dev_k}),\n",
        "    epochs=3,\n",
        "    batch_size=50#,\n",
        "    #callbacks=[tensorboard]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"bert_layer_1/bert_layer_1_module_apply_tokens/bert/encoder/Reshape_13:0\", shape=(?, ?, 768), dtype=float32)\n",
            "pred:  Tensor(\"ner_1/truediv:0\", shape=(?, ?, 75), dtype=float32)\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 56)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 56)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 56)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_layer_1 (BertLayer)        (None, None, 768)    108931396   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 256)    196864      bert_layer_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "ner (Dense)                     (None, None, 75)     19275       dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 109,147,535\n",
            "Trainable params: 71,094,859\n",
            "Non-trainable params: 38,052,676\n",
            "__________________________________________________________________________________________________\n",
            "Train on 115812 samples, validate on 15680 samples\n",
            "Epoch 1/3\n",
            "115812/115812 [==============================] - 2265s 20ms/sample - loss: 0.2041 - custom_acc_orig_tokens: 0.9559 - custom_acc_orig_non_other_tokens: 0.9559 - val_loss: 0.1781 - val_custom_acc_orig_tokens: 0.9567 - val_custom_acc_orig_non_other_tokens: 0.9567\n",
            "Epoch 2/3\n",
            "115812/115812 [==============================] - 2239s 19ms/sample - loss: 0.1395 - custom_acc_orig_tokens: 0.9643 - custom_acc_orig_non_other_tokens: 0.9643 - val_loss: 0.1533 - val_custom_acc_orig_tokens: 0.9597 - val_custom_acc_orig_non_other_tokens: 0.9597\n",
            "Epoch 3/3\n",
            "115812/115812 [==============================] - 2251s 19ms/sample - loss: 0.1228 - custom_acc_orig_tokens: 0.9669 - custom_acc_orig_non_other_tokens: 0.9669 - val_loss: 0.1439 - val_custom_acc_orig_tokens: 0.9611 - val_custom_acc_orig_non_other_tokens: 0.9611\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fab5ad7fda0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX_I5S_6fQ_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(OUTPUT_DIR + 'bert_bilou_architecture.json', 'w') as f:\n",
        "    f.write(model.to_json())\n",
        "\n",
        "model.save_weights(OUTPUT_DIR + 'bert_bilou_model_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKuJzEI7yUoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# json_file = open(OUTPUT_DIR + 'bert_bilou_architecture.json', 'r')\n",
        "# loaded_model_json = json_file.read()\n",
        "# json_file.close()\n",
        "# loaded_model = model_from_json(loaded_model_json)\n",
        "# loaded_model.load_weights(\"bert_bilou_model_weights.h5\")\n",
        "# print(\"Loaded model from disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcStUdYbfbym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reshape_predictions(yh, pr):\n",
        "#     coords = [np.where(yhh > 0)[0] for yhh in yh]\n",
        "    yh = [yhh for yhh, co in zip(yh, coords)]\n",
        "    ypr = [prr for prr, co in zip(pr, coords)]\n",
        "#     fyh = [c for row in yh for c in row]\n",
        "#     fpr = [c for row in ypr for c in row]\n",
        "    return yh, ypr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcL3qbbhxDay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#     bert_inputs_train_k, \n",
        "#     {\"ner\":labels_train_k}\n",
        "y_pred = model.predict(bert_inputs_test_k).argmax(2)\n",
        "yh = labels_test_k\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PBkSgTmz2ul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fyh_test, fpr_test = y_pred.flatten(), yh.flatten()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQPoU3KLtv-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(bert_inputs_test_k, labels_test_k)\n",
        "print(f'Model loss: {score}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgaCkIQtx7-f",
        "colab_type": "code",
        "outputId": "0af0ffcc-09a1-4319-cc1c-6dbee06abf38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Training accuracy Onto TEST : ', accuracy_score(y_pred.flatten(), yh.flatten()))\n",
        "# print('Training confusion matrix:')\n",
        "cm = confusion_matrix(fyh_test, fpr_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy Onto TEST :  0.8497023809523809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF2BSgYH1JOO",
        "colab_type": "code",
        "outputId": "9034c12e-e26c-40b9-c873-cbe033399e50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(y_pred[0])\n",
        "print(labels_test_k[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0]\n",
            "[74 72 72 72 72 72 72 72 72 72 72 72 72 72 72 75  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeVF_u1Z1z6e",
        "colab_type": "code",
        "outputId": "441b37fb-9fce-406f-821e-a938ca100853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(classification_report(fyh_test, fpr_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99    442409\n",
            "           1       0.00      0.00      0.00         0\n",
            "           2       0.08      0.50      0.14        22\n",
            "           4       0.00      0.00      0.00         0\n",
            "           5       0.00      0.00      0.00         0\n",
            "           6       0.23      0.58      0.33       456\n",
            "           7       0.41      0.50      0.45      1251\n",
            "           8       0.18      0.69      0.28       218\n",
            "           9       0.00      0.00      0.00         0\n",
            "          10       0.39      0.52      0.44       300\n",
            "          11       0.17      0.60      0.26        60\n",
            "          12       0.00      0.00      0.00         0\n",
            "          13       0.00      0.00      0.00         0\n",
            "          14       0.16      0.52      0.25       154\n",
            "          15       0.56      0.61      0.58       404\n",
            "          16       0.20      0.68      0.31       200\n",
            "          17       0.00      0.00      0.00         0\n",
            "          18       0.00      0.00      0.00         0\n",
            "          19       0.00      0.00      0.00         0\n",
            "          20       0.01      0.20      0.02         5\n",
            "          22       0.02      0.40      0.03         5\n",
            "          23       0.45      0.58      0.51       309\n",
            "          24       0.48      0.67      0.56       763\n",
            "          25       0.20      0.57      0.30        70\n",
            "          26       0.38      0.63      0.47       649\n",
            "          27       0.07      1.00      0.14         3\n",
            "          28       0.00      0.00      0.00         0\n",
            "          29       0.27      0.68      0.38       793\n",
            "          30       0.44      0.64      0.53       739\n",
            "          31       0.00      0.00      0.00         0\n",
            "          32       0.36      0.60      0.45       653\n",
            "          33       0.00      0.00      0.00         0\n",
            "          34       0.00      0.00      0.00         0\n",
            "          35       0.10      0.45      0.16        33\n",
            "          36       0.34      0.53      0.41       706\n",
            "          37       0.29      0.72      0.41       165\n",
            "          38       0.00      0.00      0.00         0\n",
            "          39       0.18      0.56      0.28        70\n",
            "          40       0.31      0.61      0.41        79\n",
            "          41       0.00      0.00      0.00         0\n",
            "          42       0.00      0.00      0.00         0\n",
            "          43       0.00      0.00      0.00         0\n",
            "          44       0.00      0.00      0.00         0\n",
            "          45       0.23      0.63      0.33       315\n",
            "          46       0.00      0.00      0.00         0\n",
            "          47       0.04      0.56      0.07         9\n",
            "          48       0.00      0.00      0.00         0\n",
            "          49       0.35      0.58      0.44       185\n",
            "          50       0.00      0.00      0.00         0\n",
            "          51       0.00      0.00      0.00         0\n",
            "          52       0.13      0.61      0.21        46\n",
            "          53       0.35      0.73      0.47       493\n",
            "          54       0.26      0.68      0.38       299\n",
            "          55       0.14      0.54      0.22       289\n",
            "          56       0.00      0.00      0.00         0\n",
            "          58       0.32      0.53      0.40       187\n",
            "          59       0.00      0.00      0.00         0\n",
            "          60       0.42      0.49      0.45       169\n",
            "          61       0.00      0.00      0.00         0\n",
            "          62       0.11      0.62      0.19        21\n",
            "          63       0.00      0.00      0.00         0\n",
            "          64       0.00      0.00      0.00         0\n",
            "          65       0.00      0.00      0.00         0\n",
            "          66       0.00      0.00      0.00         1\n",
            "          67       0.00      0.00      0.00         0\n",
            "          68       0.17      0.52      0.26       163\n",
            "          70       0.12      0.44      0.19        41\n",
            "          71       0.00      0.00      0.00         0\n",
            "          72       0.99      0.87      0.92    231418\n",
            "          73       0.00      0.00      0.00         0\n",
            "          74       0.00      0.00      0.00         0\n",
            "          75       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.93    684152\n",
            "   macro avg       0.15      0.32      0.19    684152\n",
            "weighted avg       0.98      0.93      0.96    684152\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mDfzDyA_V1_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "2fa0b10f-2b5c-4f86-fe5f-8a88416af3ca"
      },
      "source": [
        "y_pred = model.predict(bert_inputs_bios_k).argmax(2)\n",
        "yh = labels_bios_k\n",
        "fyh_bios, fpr_bios = y_pred.flatten(), yh.flatten()\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-b32e5494fc2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_inputs_bios_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0myh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_bios_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfyh_bios\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpr_bios\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP331RQgt0Dj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score_bios = model.evaluate(bert_inputs_bios_k, labels_bios_k)\n",
        "print(f'Model loss: {score_bios}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLOruQWc_-Vf",
        "colab_type": "code",
        "outputId": "2a449afc-d4b0-41be-cdc7-42e19a786cdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "print('Training accuracy DEF14A : ', accuracy_score(fyh_bios, fpr_bios))\n",
        "# print('Training confusion matrix:')\n",
        "cm = confusion_matrix(fyh_test, fpr_test)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-0080ea984753>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training accuracy DEF14A : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfyh_bios\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpr_bios\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# print('Training confusion matrix:')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfyh_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpr_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fyh_bios' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LBo2bs3BKdA",
        "colab_type": "code",
        "outputId": "212c3f2b-4283-484a-ab3a-648b90137bcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        }
      },
      "source": [
        "print(classification_report(fyh_bios, fpr_bios))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.96      0.98     23246\n",
            "           1       0.00      0.00      0.00         0\n",
            "           5       0.00      0.00      0.00         0\n",
            "           6       0.13      0.57      0.22       199\n",
            "           7       0.31      0.47      0.38       796\n",
            "           8       0.07      0.57      0.13        28\n",
            "          10       0.50      1.00      0.67         1\n",
            "          13       0.00      0.00      0.00         0\n",
            "          14       0.14      0.60      0.23         5\n",
            "          15       1.00      0.50      0.67         4\n",
            "          16       0.07      0.41      0.12        37\n",
            "          17       0.00      0.00      0.00         0\n",
            "          19       0.00      0.00      0.00         0\n",
            "          23       0.50      1.00      0.67         1\n",
            "          24       0.86      0.83      0.85       103\n",
            "          25       0.00      0.00      0.00         2\n",
            "          26       0.55      0.50      0.52       111\n",
            "          29       0.17      0.19      0.18        36\n",
            "          30       0.09      0.40      0.15        67\n",
            "          32       0.20      0.49      0.29       136\n",
            "          36       0.24      0.49      0.32       156\n",
            "          37       0.35      0.68      0.46        38\n",
            "          45       0.00      0.00      0.00         1\n",
            "          49       0.33      0.33      0.33         3\n",
            "          50       0.00      0.00      0.00         0\n",
            "          52       0.00      0.00      0.00         0\n",
            "          53       0.67      0.92      0.77       182\n",
            "          54       0.50      0.25      0.33         4\n",
            "          55       0.07      0.42      0.12       149\n",
            "          56       0.00      0.00      0.00         0\n",
            "          58       0.33      1.00      0.50         1\n",
            "          64       0.00      0.00      0.00         0\n",
            "          67       0.00      0.00      0.00         0\n",
            "          68       0.10      0.50      0.17         4\n",
            "          72       0.95      0.72      0.82     14338\n",
            "          73       0.00      0.00      0.00         0\n",
            "          74       0.00      0.00      0.00         0\n",
            "          75       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.85     39648\n",
            "   macro avg       0.24      0.36      0.26     39648\n",
            "weighted avg       0.94      0.85      0.89     39648\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}