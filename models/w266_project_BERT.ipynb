{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w266_project_BERT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDxMc4BpSLbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from time import time\n",
        "import io\n",
        "import re\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support,classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.python.keras.layers import Lambda\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.layers import Dense, TimeDistributed\n",
        "from tensorflow.python.tools import inspect_checkpoint as chkp\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX-ccqNvSlck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install tf-nightly-2.0-preview\n",
        "\n",
        "# !pip install \"tensorflow>=1.7.0\"\n",
        "\n",
        "# !pip install tensorflow-hub\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jByW-tjqT3xD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6t4yTljTcUm",
        "colab_type": "code",
        "outputId": "ba952822-3e99-46dc-b987-0f687e13454b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq1ONMDTUelM",
        "colab_type": "code",
        "outputId": "67694b86-6a49-4e7e-a2c5-4cd1a4e8b16c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "__dir__ = '/content/drive/My Drive/Colab Notebooks/'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dusZrEISUua8",
        "colab_type": "code",
        "outputId": "e574b205-6c28-4991-dd30-2a16bb2c83f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHWOzxauBOn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zUnS62IDi1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr7rVYUWEj53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRMS8LBpU8sl",
        "colab_type": "code",
        "outputId": "c3b1ce2c-aa42-4d46-bf91-9ef8ed890b34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0803 20:38:53.018659 140093742360448 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-tUZ9HFIPQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !ls /content/gdrive/My\\ Drive/Colab\\ Notebooks/custom_module/bert-ner/*.py\n",
        "# !cat /content/gdrive/My\\ Drive/Colab\\ Notebooks/custom_module/bert_ner/run_classifier_ner.py\n",
        "# import sys\n",
        "# sys.path.append('/content/gdrive/My\\ Drive/Colab\\ Notebooks/custom_module/bert_ner/')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esAOy2hlVv4K",
        "colab_type": "code",
        "outputId": "07845541-a918-4dce-a0ff-3718a3e6f953",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "OUTPUT_DIR = '/content/drive/My Drive/Colab Notebooks/Model_Output/'\n",
        "tf.io.gfile.makedirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: /content/drive/My Drive/Colab Notebooks/Model_Output/ *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFLH3M0vf70K",
        "colab_type": "code",
        "outputId": "ada6530c-cef7-4ba8-c8e2-db8a97181c6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1'\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info =  bert_module(signature='tokenization_info', as_dict=True)\n",
        "#     print(tokenization_info)\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "      vocab_file = sess.run(tokenization_info['vocab_file'])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(vocab_file=vocab_file)\n",
        "\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0803 20:38:56.904978 140093742360448 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Er-1iMvtpf",
        "colab_type": "code",
        "outputId": "e8ef9771-5090-4f6e-bf52-7f1f4f78ebbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'here',\n",
              " \"'\",\n",
              " 's',\n",
              " 'an',\n",
              " 'example',\n",
              " 'of',\n",
              " 'using',\n",
              " 'the',\n",
              " 'be',\n",
              " '##rt',\n",
              " 'token',\n",
              " '##izer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKp5gC64wyyW",
        "colab_type": "code",
        "outputId": "f661c948-8fdb-4a4a-eb6c-2b9ec3417e4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "def checkPrior(blah):\n",
        "\tif (blah.prior is None or blah.prior is np.NaN) and (blah.prior_pos is None or blah.prior_pos is np.NaN):\n",
        "\t\treturn True\n",
        "\telse:\n",
        "\t\treturn False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_onto_data_vectors(data_type):\n",
        "  data_type_dict = {'test' : '/content/drive/My Drive/Colab Notebooks/data/onto.test.ner',\n",
        "                   'dev' : '/content/drive/My Drive/Colab Notebooks/data/onto.development.ner',\n",
        "                   'train' : '/content/drive/My Drive/Colab Notebooks/data/onto.train.ner',\n",
        "                   'sample': '/content/drive/My Drive/Colab Notebooks/data/onto.train.ner.sample'}\n",
        "  header = 0\n",
        "  if 'onto' in data_type_dict[data_type]:\n",
        "    header = None\n",
        "  data = pd.read_csv(data_type_dict[data_type], sep=\"\\t\",  quoting=csv.QUOTE_NONE, header=header, skip_blank_lines=False, engine='python', names =['token', 'pos', 'tree', 'label'])\n",
        "\n",
        "  # see if prior row was a newline\n",
        "  data['prior'] = data.token.shift(1)\n",
        "  data['prior_pos'] = data.pos.shift(1)\n",
        "  # drop empty rows\n",
        "  data = data.loc[~data.token.isnull()]\n",
        "  data.prior = data.apply(checkPrior, axis=1)\n",
        "  data['phrase'] = data.prior.cumsum()\n",
        "  df = data.groupby('phrase').agg({'token':list, 'label': list})\n",
        "\n",
        "  df['token'] = df['token'].apply(lambda x: ' '.join(x))\n",
        "  X = df['token'].tolist()\n",
        "  y = df['label'].tolist()\n",
        "  return X, y\n",
        "\n",
        "X, y = create_onto_data_vectors('train')\n",
        "\n",
        "X_dev, y_dev = create_onto_data_vectors('dev')\n",
        "\n",
        "X_test, y_test = create_onto_data_vectors('test')\n",
        "\n",
        "\n",
        "\n",
        "X_train, y_train = create_onto_data_vectors('train')\n",
        "label_list = list(set([label for target_list in y_train for label in target_list])) + ['[CLS]', '[SEP]']\n",
        "# print(label_list)\n",
        "label_map = {label : i for i, label in enumerate(label_list,1)}\n",
        "print(label_map)\n",
        "print(f'Length of labels - {len(label_list)}')\n",
        "\n",
        "\n",
        "X_dev, y_dev = create_onto_data_vectors('dev')\n",
        "\n",
        "print(len(X_train))\n",
        "print(len(y_train))\n",
        "print(y_train[0])\n",
        "print(X_train[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'B-WORK_OF_ART': 1, 'I-EVENT': 2, 'I-GPE': 3, 'I-TIME': 4, 'I-QUANTITY': 5, 'B-NORP': 6, 'B-PERCENT': 7, 'O': 8, 'I-NORP': 9, 'I-WORK_OF_ART': 10, 'I-FAC': 11, 'I-LANGUAGE': 12, 'B-ORDINAL': 13, 'B-PERSON': 14, 'B-PRODUCT': 15, 'I-LOC': 16, 'B-TIME': 17, 'B-ORG': 18, 'I-DATE': 19, 'I-PERSON': 20, 'B-LANGUAGE': 21, 'B-LAW': 22, 'B-QUANTITY': 23, 'I-PERCENT': 24, 'B-MONEY': 25, 'B-CARDINAL': 26, 'B-DATE': 27, 'I-PRODUCT': 28, 'B-FAC': 29, 'I-MONEY': 30, 'B-EVENT': 31, 'I-ORG': 32, 'B-LOC': 33, 'I-LAW': 34, 'B-GPE': 35, 'I-ORDINAL': 36, 'I-CARDINAL': 37, '[CLS]': 38, '[SEP]': 39}\n",
            "Length of labels - 39\n",
            "115812\n",
            "115812\n",
            "['O', 'O', 'O', 'O', 'O']\n",
            "What kind of memory ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhaOIQWJtvy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# file = pd.ExcelFile('/content/drive/My Drive/Colab Notebooks/data/test_data_no_title.xlsx')\n",
        "\n",
        "# df = file.parse('Sheet2', header=None)\n",
        "# df_words = df[0]\n",
        "# df_ner = df[1]\n",
        "\n",
        "# df_words.loc[df_words.isnull()] = df_words.loc[df_words.isnull()].apply(lambda x: 'BREAK')\n",
        "# df_ner.loc[df_ner.isnull()] = df_ner.loc[df_ner.isnull()].apply(lambda x: 'BREAK')\n",
        "\n",
        "\n",
        "# words_list = df_words.tolist()\n",
        "# ner_list = df_ner.tolist()\n",
        "\n",
        "# word_seq = []\n",
        "# word_sentences = []\n",
        "# for count, word in enumerate(words_list):\n",
        "#     if word != 'BREAK':\n",
        "#         word_seq.append(str(word))\n",
        "        \n",
        "#     else:\n",
        "#         word_sentences.append(' '.join(word_seq))\n",
        "#         word_seq = []\n",
        "#         continue\n",
        "# print(f'total tokens - {count}')\n",
        "\n",
        "# ner_seq = []\n",
        "# ner_sentences = []\n",
        "# for count_tags, ner in enumerate(ner_list):\n",
        "#     if ner != 'BREAK':\n",
        "#         ner_seq.append(ner)\n",
        "#     else:\n",
        "#         ner_sentences.append(ner_seq)\n",
        "#         ner_seq = []\n",
        "#         continue\n",
        "\n",
        "# print(f'Total tags - {count_tags}')\n",
        "\n",
        "# #print(word_sentences[0])    \n",
        "# #print(ner_sentences[0])    \n",
        "\n",
        "# X_test, y_test = word_sentences, ner_sentences\n",
        "\n",
        "# print(len(X_test))\n",
        "# print(len(y_test))\n",
        "\n",
        "# print(X_test[0])\n",
        "# print(y_test[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dA6uqxNHVST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FILE_PATH = '/content/drive/My Drive/Colab Notebooks/data/bios-tagged-final-flat.csv'\n",
        "\n",
        "header = 0\n",
        "if 'onto' in FILE_PATH:\n",
        "\theader = None\n",
        "data = pd.read_csv(FILE_PATH, sep=\"\\t\",  quoting=csv.QUOTE_NONE, header=header, skip_blank_lines=False, engine='python', names =['token', 'pos', 'tree', 'label'])\n",
        "\n",
        "# see if prior row was a newline\n",
        "data['prior'] = data.token.shift(1)\n",
        "data['prior_pos'] = data.pos.shift(1)\n",
        "# drop empty rows\n",
        "data = data.loc[~data.token.isnull()]\n",
        "data.prior = data.apply(checkPrior, axis=1)\n",
        "data['phrase'] = data.prior.cumsum()\n",
        "\n",
        "#aggregate by sentence\n",
        "bios_df = data.groupby('phrase').agg({'token':list, 'label': list})\n",
        "bios_df['token'] = bios_df['token'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "X_bios = bios_df['token'].tolist()\n",
        "y_bios = bios_df['label'].tolist()\n",
        "\n",
        "y_bios_no_title = [['O' if label in ['B-TITLE','U-TITLE','L-TITLE','I-TITLE'] else label for label in seq] for seq in y_bios]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByGZHykEIURZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "71f7a54f-b107-4f16-f115-21eecbf35eb4"
      },
      "source": [
        "print(X_bios[0])\n",
        "print(y_bios_no_title[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jeffrey P. Bezos , age 55 , has been Chairman of the Board since founding the Company in 1994 and Chief Executive Officer since May 1996 .\n",
            "['B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'B-ORG', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SzKe02ugQKE",
        "colab_type": "code",
        "outputId": "f4eeb5a8-9c68-4a86-9b48-5e0c7f7bd462",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "#transfer sentences to txt file, one ssentence per line, for ELMO encoder\n",
        "\n",
        "# os.chdir('/content/drive/My Drive/Colab Notebooks/data/')\n",
        "\n",
        "# test_list = [['a', 'b', 'c', '.'],['1','2']]\n",
        "\n",
        "# with open('test.txt', 'w+') as file:\n",
        "#   for line in test_list:\n",
        "#     file.write(' '.join(line) + '\\n')\n",
        "\n",
        "# with open('X_train-MICRO.txt', 'w') as file:\n",
        "#   for indx, seq in enumerate(X_train):\n",
        "#     if len(seq) > 0 and indx < 100:\n",
        "#       file.write(seq + '\\n')\n",
        "\n",
        "\n",
        "label_map = {label : i for i, label in enumerate(label_list,1)}\n",
        "print(label_map)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'B-WORK_OF_ART': 1, 'I-EVENT': 2, 'I-GPE': 3, 'I-TIME': 4, 'I-QUANTITY': 5, 'B-NORP': 6, 'B-PERCENT': 7, 'O': 8, 'I-NORP': 9, 'I-WORK_OF_ART': 10, 'I-FAC': 11, 'I-LANGUAGE': 12, 'B-ORDINAL': 13, 'B-PERSON': 14, 'B-PRODUCT': 15, 'I-LOC': 16, 'B-TIME': 17, 'B-ORG': 18, 'I-DATE': 19, 'I-PERSON': 20, 'B-LANGUAGE': 21, 'B-LAW': 22, 'B-QUANTITY': 23, 'I-PERCENT': 24, 'B-MONEY': 25, 'B-CARDINAL': 26, 'B-DATE': 27, 'I-PRODUCT': 28, 'B-FAC': 29, 'I-MONEY': 30, 'B-EVENT': 31, 'I-ORG': 32, 'B-LOC': 33, 'I-LAW': 34, 'B-GPE': 35, 'I-ORDINAL': 36, 'I-CARDINAL': 37, '[CLS]': 38, '[SEP]': 39}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNFvbqxgjl9H",
        "colab_type": "code",
        "outputId": "7c0ba765-7e09-4880-8be5-38ea09648850",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(X_train[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What kind of memory ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b4bGJ9xiEdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMPyjNUf0T5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def conv_to_df(X, y):\n",
        "\n",
        "  df = pd.DataFrame([X,y]).T\n",
        "  df.columns = ['X', 'y']\n",
        "#   print (type(df.loc[0, 'X']))\n",
        "#   print (type(df.loc[0, 'y']))\n",
        "#   df['y'] = df['y'].str.strip('[]')\n",
        "#   print(df.head())\n",
        "  return df\n",
        "\n",
        "train = conv_to_df(X_train, y_train)\n",
        "dev = conv_to_df(X_dev, y_dev)\n",
        "test = conv_to_df(X_test, y_test)\n",
        "bios = conv_to_df(X_bios, y_bios_no_title)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz8tCXSI3e1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_to_txt = train['X']\n",
        "\n",
        "# dev_to_txt = dev['X']\n",
        "\n",
        "# # train_to_txt.to_csv('/content/drive/My Drive/Colab Notebooks/data/X_train.csv', header=None, index=None,\n",
        "# #                     quoting=csv.QUOTE_MINIMAL, na_rep='', escapechar='\\\\')\n",
        "# os.chdir('/content/drive/My Drive/Colab Notebooks/data/')\n",
        "\n",
        "# # pd.set_option(\"display.max_colwidth\", 256)\n",
        "\n",
        "# with open('test.txt', 'w+') as f:\n",
        "#     train_to_txt.to_string(f, index=False)\n",
        "\n",
        "# # dev_to_txt.to_csv('/content/drive/My Drive/Colab Notebooks/data/X_dev.txt', header=None, index=None, quoting=csv.QUOTE_NONE)\n",
        "\n",
        "# dev_to_txt.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05u8jaJLQOej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id, valid_ids=None, label_mask=None):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "        self.valid_ids = valid_ids\n",
        "        self.label_mask = label_mask\n",
        "\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    label_map = {label : i for i, label in enumerate(label_list,1)}\n",
        "    \n",
        "    features = []\n",
        "    for (ex_index,example) in enumerate(examples):\n",
        "        textlist = example.text_a.split(' ')\n",
        "        labellist = example.label\n",
        "        if len(textlist) != len(labellist):\n",
        "          continue\n",
        "        else:\n",
        "          tokens = []\n",
        "          labels = []\n",
        "          valid = []\n",
        "          label_mask = []\n",
        "          for i, word in enumerate(textlist): \n",
        "              token = tokenizer.tokenize(word)\n",
        "              tokens.extend(token)\n",
        "\n",
        "              label_1 = labellist[i]\n",
        "              for m in range(len(token)):\n",
        "                  if m == 0:\n",
        "                      labels.append(label_1)\n",
        "                      valid.append(1)\n",
        "                      label_mask.append(1)\n",
        "                  else:\n",
        "                      valid.append(0)\n",
        "          if len(tokens) >= max_seq_length - 1:\n",
        "              tokens = tokens[0:(max_seq_length - 2)]\n",
        "              labels = labels[0:(max_seq_length - 2)]\n",
        "              valid = valid[0:(max_seq_length - 2)]\n",
        "              label_mask = label_mask[0:(max_seq_length - 2)]\n",
        "          ntokens = []\n",
        "          segment_ids = []\n",
        "          label_ids = []\n",
        "          ntokens.append(\"[CLS]\")\n",
        "          segment_ids.append(0)\n",
        "          valid.insert(0,1)\n",
        "          label_mask.insert(0,1)\n",
        "          label_ids.append(label_map[\"[CLS]\"])\n",
        "          for i, token in enumerate(tokens):\n",
        "              ntokens.append(token)\n",
        "              segment_ids.append(0)\n",
        "              if len(labels) > i:\n",
        "                  label_ids.append(label_map[labels[i]])\n",
        "          ntokens.append(\"[SEP]\")\n",
        "          segment_ids.append(0)\n",
        "          valid.append(1)\n",
        "          label_mask.append(1)\n",
        "          label_ids.append(label_map[\"[SEP]\"])\n",
        "          input_ids = tokenizer.convert_tokens_to_ids(ntokens)\n",
        "          input_mask = [1] * len(input_ids)\n",
        "          label_mask = [1] * len(label_ids)\n",
        "          while len(input_ids) < max_seq_length:\n",
        "              input_ids.append(0)\n",
        "              input_mask.append(0)\n",
        "              segment_ids.append(0)\n",
        "              label_ids.append(0)\n",
        "              valid.append(1)\n",
        "              label_mask.append(0)\n",
        "          while len(label_ids) < max_seq_length:\n",
        "              label_ids.append(0)\n",
        "              label_mask.append(0)\n",
        "          assert len(input_ids) == max_seq_length\n",
        "          assert len(input_mask) == max_seq_length\n",
        "          assert len(segment_ids) == max_seq_length\n",
        "          assert len(label_ids) == max_seq_length\n",
        "          assert len(valid) == max_seq_length\n",
        "          assert len(label_mask) == max_seq_length\n",
        "\n",
        "          if ex_index < 5:\n",
        "              logger.info(\"*** Example ***\")\n",
        "              logger.info(\"guid: %s\" % (example.guid))\n",
        "              logger.info(\"tokens: %s\" % \" \".join(\n",
        "                      [str(x) for x in tokens]))\n",
        "              logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "              logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "              logger.info(\n",
        "                      \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "#               logger.info(\"label: %s (id = %d)\" % (example.label, label_ids))\n",
        "\n",
        "          features.append(\n",
        "                  InputFeatures(input_ids=input_ids,\n",
        "                                input_mask=input_mask,\n",
        "                                segment_ids=segment_ids,\n",
        "                                label_id=label_ids,\n",
        "                                valid_ids=valid,\n",
        "                                label_mask=label_mask))\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvIsGm6cyz6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x['X'], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x['y']), axis = 1)\n",
        "\n",
        "dev_InputExamples = dev.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x['X'], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x['y']), axis = 1)\n",
        "\n",
        "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x['X'], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x['y']), axis = 1)\n",
        "\n",
        "bios_InputExamples = bios.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x['X'], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x['y']), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3Y2Qc4LzUy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQ_LENGTH = 56\n",
        "max_length = MAX_SEQ_LENGTH\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI5XZmsNsXbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i13GFhBiD9ex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_features = convert_examples_to_features(dev_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGrT45zqzLuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_features = convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSaR3hbqJSTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bios_features = convert_examples_to_features(bios_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6CunX7l9F_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainSentence_ids = []\n",
        "trainMasks = []\n",
        "trainSequence_ids = []\n",
        "\n",
        "devSentence_ids = []\n",
        "devMasks = []\n",
        "devSequence_ids = []\n",
        "\n",
        "testSentence_ids = []\n",
        "testMasks = []\n",
        "testSequence_ids = []\n",
        "\n",
        "biosSentence_ids = []\n",
        "biosMasks = []\n",
        "biosSequence_ids = []\n",
        "\n",
        "\n",
        "nerLabels_train =[]\n",
        "nerLabels_dev = []\n",
        "nerLabels_test = []\n",
        "nerLabels_bios = []\n",
        "\n",
        "\n",
        "\n",
        "bert_inputs_train = np.array([[f.input_ids for f in train_features], [f.input_mask for f in train_features],[f.segment_ids for f in train_features]])\n",
        "labels_train = [f.label_id for f in train_features]\n",
        "num_sentences_train = len([f.input_ids for f in train_features])\n",
        "# print(num_sentences)\n",
        "for example in range(num_sentences_train):\n",
        "    trainSentence_ids.append(bert_inputs_train[0][example])\n",
        "    trainMasks.append(bert_inputs_train[1][example])\n",
        "    trainSequence_ids.append(bert_inputs_train[2][example])\n",
        "    nerLabels_train.append(labels_train[example])\n",
        "\n",
        "    \n",
        "# labels_train = [to_categorical(f.label_id) for f in train_features]\n",
        "\n",
        "\n",
        "bert_inputs_dev = [[f.input_ids for f in dev_features], [f.input_mask for f in dev_features],[f.segment_ids for f in dev_features]]\n",
        "labels_dev = [f.label_id for f in dev_features]\n",
        "num_sentences_dev = len([f.input_ids for f in dev_features])\n",
        "for example in range(num_sentences_dev):\n",
        "    devSentence_ids.append(bert_inputs_dev[0][example])\n",
        "    devMasks.append(bert_inputs_dev[1][example])\n",
        "    devSequence_ids.append(bert_inputs_dev[2][example])\n",
        "    nerLabels_dev.append(labels_dev[example])\n",
        "\n",
        "    \n",
        "bert_inputs_test = np.array([[f.input_ids for f in test_features], [f.input_mask for f in test_features],[f.segment_ids for f in test_features]])\n",
        "labels_test = [f.label_id for f in test_features]\n",
        "num_sentences_test = len([f.input_ids for f in test_features])\n",
        "# print(num_sentences)\n",
        "for example in range(num_sentences_test):\n",
        "    testSentence_ids.append(bert_inputs_test[0][example])\n",
        "    testMasks.append(bert_inputs_test[1][example])\n",
        "    testSequence_ids.append(bert_inputs_test[2][example])\n",
        "    nerLabels_test.append(labels_test[example])    \n",
        "    \n",
        "    \n",
        "bert_inputs_bios = np.array([[f.input_ids for f in bios_features], [f.input_mask for f in bios_features],[f.segment_ids for f in bios_features]])\n",
        "labels_bios = [f.label_id for f in bios_features]\n",
        "num_sentences_bios = len([f.input_ids for f in bios_features])\n",
        "# print(num_sentences)\n",
        "for example in range(num_sentences_bios):\n",
        "    biosSentence_ids.append(bert_inputs_bios[0][example])\n",
        "    biosMasks.append(bert_inputs_bios[1][example])\n",
        "    biosSequence_ids.append(bert_inputs_bios[2][example])\n",
        "    nerLabels_bios.append(labels_bios[example])    \n",
        "        \n",
        "    \n",
        "X_train = np.array([trainSentence_ids,trainMasks,trainSequence_ids])\n",
        "X_dev = np.array([devSentence_ids,devMasks,devSequence_ids])\n",
        "X_test = np.array([testSentence_ids,testMasks,testSequence_ids])\n",
        "X_bios = np.array([biosSentence_ids,biosMasks,biosSequence_ids])\n",
        "\n",
        "\n",
        "nerLabels_train = np.array(nerLabels_train)\n",
        "nerLabels_dev = np.array(nerLabels_dev)\n",
        "nerLabels_test = np.array(nerLabels_test)\n",
        "nerLabels_bios = np.array(nerLabels_bios)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# labels_dev = [to_categorical(f.label_id) for f in dev_features]\n",
        "\n",
        "#                                 input_ids=input_ids,\n",
        "#                                 input_mask=input_mask,\n",
        "#                                 segment_ids=segment_ids,\n",
        "#                                 label_id=label_ids,\n",
        "#                                 valid_ids=valid,\n",
        "#                                 label_mask=label_mask))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XObyWufETv6",
        "colab_type": "code",
        "outputId": "2e66db4b-18d9-4626-9054-1038d4050882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 12217, 56)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dzh3NvGjfH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k_start = 0\n",
        "k_end = -1\n",
        "\n",
        "if k_end == -1:\n",
        "    k_end_train = X_train[0].shape[0]\n",
        "    k_end_dev = X_dev[0].shape[0]\n",
        "    k_end_test = X_test[0].shape[0]\n",
        "else:\n",
        "    k_end_train = k_end_dev = k_end\n",
        "    \n",
        "\n",
        "\n",
        "bert_inputs_train_k = [X_train[0][k_start:k_end_train], X_train[1][k_start:k_end_train], \n",
        "                       X_train[2][k_start:k_end_train]]\n",
        "bert_inputs_dev_k = [X_dev[0][k_start:k_end_dev], X_dev[1][k_start:k_end_dev], \n",
        "                      X_dev[2][k_start:k_end_dev]]\n",
        "bert_inputs_test_k = [X_test[0][k_start:k_end_test], X_test[1][k_start:k_end_test], \n",
        "                      X_test[2][k_start:k_end_test]]\n",
        "bert_inputs_bios_k = [X_bios[0][k_start:k_end_test], X_bios[1][k_start:k_end_test], \n",
        "                      X_bios[2][k_start:k_end_test]]\n",
        "\n",
        "\n",
        "\n",
        "labels_train_k = nerLabels_train[k_start:k_end_train]\n",
        "labels_dev_k = nerLabels_dev[k_start:k_end_dev]\n",
        "labels_test_k = nerLabels_test[k_start:k_end_test]\n",
        "labels_bios_k = nerLabels_bios[k_start:k_end_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpcvUErNV0rY",
        "colab_type": "code",
        "outputId": "59ecd592-3f2f-41fa-bea9-0a2c8c0c21d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "for item in bert_inputs_train_k:\n",
        "  print(item.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(115812, 56)\n",
            "(115812, 56)\n",
            "(115812, 56)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paSYTF_aNVs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsdEnB9ZQ46D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bert_inputs_train[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YET3fTq5RIFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbbGv2bSU3nC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYfrPm_3YdF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # check_labels = [f.label_id for f in train_features]\n",
        "# print(x_label_mask[0])\n",
        "# # print(labels_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwS--eHBEEWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(x_valid_ids[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6quxfwadbqtV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Create BERT layer, following https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b\n",
        "    init:  initialize layer. Specify various parameters regarding output types and dimensions. Very important is\n",
        "           to set the number of trainable layers.\n",
        "    build: build the layer based on parameters\n",
        "    call:  call the BERT layer within a model\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        n_fine_tune_layers=10,\n",
        "        pooling=\"sequence\",\n",
        "        bert_url=\"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.n_fine_tune_layers = n_fine_tune_layers\n",
        "        self.trainable = True\n",
        "        self.output_size = 768\n",
        "        self.pooling = pooling\n",
        "        self.bert_url = bert_url\n",
        "\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bert = hub.Module(\n",
        "            self.bert_url, trainable=self.trainable, name=f\"{self.name}_module\"\n",
        "        )\n",
        "\n",
        "        # Remove unused layers\n",
        "        trainable_vars = self.bert.variables\n",
        "        trainable_vars = [\n",
        "                var\n",
        "                for var in trainable_vars\n",
        "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
        "            ]\n",
        "        trainable_layers = []\n",
        "\n",
        "\n",
        "        # Select how many layers to fine tune\n",
        "        for i in range(self.n_fine_tune_layers):\n",
        "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
        "\n",
        "        # Update trainable vars to contain only the specified layers\n",
        "        trainable_vars = [\n",
        "            var\n",
        "            for var in trainable_vars\n",
        "            if any([l in var.name for l in trainable_layers])\n",
        "        ]\n",
        "\n",
        "        # Add to trainable weights\n",
        "        for var in trainable_vars:\n",
        "            self._trainable_weights.append(var)\n",
        "\n",
        "        for var in self.bert.variables:\n",
        "            if var not in self._trainable_weights:\n",
        "                self._non_trainable_weights.append(var)\n",
        "\n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        input_ids, input_mask, segment_ids = inputs\n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
        "                \"sequence_output\"\n",
        "            ]\n",
        "\n",
        "        mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ27Kw2Kmo9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def variable_summaries(var):\n",
        "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
        "    with tf.name_scope('summaries'):\n",
        "        mean = tf.reduce_mean(var)\n",
        "        tf.summary.scalar('mean', mean)\n",
        "        with tf.name_scope('stddev'):\n",
        "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "        tf.summary.scalar('stddev', stddev)\n",
        "        tf.summary.scalar('max', tf.reduce_max(var))\n",
        "        tf.summary.scalar('min', tf.reduce_min(var))\n",
        "        tf.summary.histogram('histogram', var)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzhdO01D4-pW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    calculate loss function explicitly, filtering out 'extra inserted labels'\n",
        "    \n",
        "    y_true: Shape: (batch x (max_length + 1) )\n",
        "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
        "    \n",
        "    returns:  cost\n",
        "    \"\"\"\n",
        "\n",
        "    #get labels and predictions\n",
        "    \n",
        "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int32)),[-1])\n",
        "#     print(y_label.eval())\n",
        "    mask = (y_label < 38)   # This mask is used to remove all tokens that do not correspond to the original base text.\n",
        "\n",
        "    y_label_masked = tf.boolean_mask(y_label, mask)  # mask the labels\n",
        "    \n",
        "    y_flat_pred = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float32)),[-1, 39])\n",
        "#     print(y_flat_pred.eval())\n",
        "    y_flat_pred_masked = tf.boolean_mask(y_flat_pred, mask) # mask the predictions\n",
        "    \n",
        "    return tf.reduce_mean(sparse_categorical_crossentropy(y_label_masked, y_flat_pred_masked,from_logits=False ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDj9h-izI5Dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_acc_orig_tokens(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    calculate loss dfunction filtering out also the newly inserted labels\n",
        "    \n",
        "    y_true: Shape: (batch x (max_length) )\n",
        "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
        "    \n",
        "    returns: accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    #get labels and predictions\n",
        "    \n",
        "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
        "    \n",
        "    mask = (y_label < 38)\n",
        "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
        "    \n",
        "    y_predicted = tf.math.argmax(input = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
        "                                                    [-1, 39]), axis=1)\n",
        "    \n",
        "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
        "\n",
        "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRV1UGunI56x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_acc_orig_non_other_tokens(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    calculate loss dfunction explicitly filtering out also the 'Other'- labels\n",
        "    \n",
        "    y_true: Shape: (batch x (max_length) )\n",
        "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
        "    \n",
        "    returns: accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    #get labels and predictions\n",
        "    \n",
        "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
        "    \n",
        "    mask = (y_label < 38)\n",
        "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
        "    \n",
        "    y_predicted = tf.math.argmax(input = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
        "                                                    [-1, 39]), axis=1)\n",
        "    \n",
        "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
        "\n",
        "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QnN7t_VVkBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adam_customized = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.91, beta_2=0.999, epsilon=None, decay=0.1, amsgrad=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5-UcahlCpw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# label_list = {'I-EVENT': 1, 'B-LOC': 2, 'I-MONEY': 3, 'I-FAC': 4, 'B-FAC': 5, 'I-LANGUAGE': 6, 'B-DATE': 7, 'I-WORK_OF_ART': 8, 'B-PRODUCT': 9, 'B-TIME': 10, 'I-GPE': 11, 'B-ORG': 12, 'B-CARDINAL': 13, 'I-PERSON': 14, 'B-GPE': 15, 'B-QUANTITY': 16, 'I-NORP': 17, 'I-ORDINAL': 18, 'B-PERSON': 19, 'B-WORK_OF_ART': 20, 'I-PERCENT': 21, 'I-ORG': 22, 'B-MONEY': 23, 'I-PRODUCT': 24, 'I-QUANTITY': 25, 'I-LAW': 26, 'B-EVENT': 27, 'I-LOC': 28, 'B-ORDINAL': 29, 'B-LANGUAGE': 30, 'B-PERCENT': 31, 'O': 32, 'I-DATE': 33, 'I-TIME': 34, 'B-LAW': 35, 'I-CARDINAL': 36, 'B-NORP': 37, '[CLS]': 38, '[SEP]': 39}\n",
        "\n",
        "# y_true = tf.constant([[0],[0]])\n",
        "\n",
        "# y_pred = tf.constant([\n",
        "#     [0.6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "#     [0.6,0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "# ])\n",
        "\n",
        "# sess = tf.InteractiveSession()\n",
        "# sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# print(custom_loss(y_true, y_pred).eval())\n",
        "\n",
        "# sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0P9H7v6dOYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def ner_model(max_input_length, train_layers, optimizer):\n",
        "    \"\"\"\n",
        "    Implementation of NER model\n",
        "    \n",
        "    variables:\n",
        "        max_input_length: number of tokens (max_length + 1)\n",
        "        train_layers: number of layers to be retrained\n",
        "        optimizer: optimizer to be used\n",
        "    \n",
        "    returns: model\n",
        "    \"\"\"\n",
        "    \n",
        "    in_id = tf.keras.layers.Input(shape=(max_length,), name=\"input_ids\")\n",
        "    in_mask = tf.keras.layers.Input(shape=(max_length,), name=\"input_masks\")\n",
        "    in_segment = tf.keras.layers.Input(shape=(max_length), name=\"segment_ids\")\n",
        "    \n",
        "    \n",
        "    bert_inputs = [in_id, in_mask, in_segment]\n",
        "    \n",
        "    bert_sequence = BertLayer(n_fine_tune_layers=train_layers)(bert_inputs)\n",
        "    \n",
        "    print(bert_sequence)\n",
        "    \n",
        "    dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(bert_sequence)\n",
        "    \n",
        "#     dense = tf.keras.layers.Dropout(rate=0.2)(dense)\n",
        "    \n",
        "    pred = tf.keras.layers.Dense(39, activation='softmax', name='ner')(dense)\n",
        "     \n",
        "    print('pred: ', pred)\n",
        "    \n",
        "    ## Prepare for multipe loss functions, although not used here\n",
        "    \n",
        "    losses = {\n",
        "        \"ner\": custom_loss,\n",
        "        }\n",
        "    lossWeights = {\"ner\": 1.0\n",
        "                  }\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
        "\n",
        "#     model.compile(loss=losses, optimizer=optimizer, metrics=[custom_acc_orig_tokens, \n",
        "#                                                           custom_acc_orig_non_other_tokens])\n",
        "    \n",
        "    model.compile(loss= losses, optimizer=optimizer, metrics=[custom_acc_orig_tokens, \n",
        "                                                          custom_acc_orig_non_other_tokens])\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def initialize_vars(sess):\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    K.set_session(sess)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EKEy4lkrtzW",
        "colab_type": "code",
        "outputId": "661bfb14-f4ea-46e8-a549-bdba100fa1d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Start session\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "model = ner_model(max_length , train_layers=10, optimizer = adam_customized)\n",
        "\n",
        "# Instantiate variables\n",
        "initialize_vars(sess)\n",
        "\n",
        "\n",
        "\n",
        "# tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
        "\n",
        "model.fit(\n",
        "    bert_inputs_train_k, \n",
        "    {\"ner\":labels_train_k},\n",
        "    validation_data=(bert_inputs_dev_k, {\"ner\":labels_dev_k}),\n",
        "    epochs=3,\n",
        "    batch_size=50 #,\n",
        "    #callbacks=[tensorboard]\n",
        ")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"bert_layer_1/bert_layer_1_module_apply_tokens/bert/encoder/Reshape_13:0\", shape=(?, ?, 768), dtype=float32)\n",
            "pred:  Tensor(\"ner_1/truediv:0\", shape=(?, ?, 39), dtype=float32)\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 56)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 56)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 56)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_layer_1 (BertLayer)        (None, None, 768)    108931396   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 256)    196864      bert_layer_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "ner (Dense)                     (None, None, 39)     10023       dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 109,138,283\n",
            "Trainable params: 71,085,607\n",
            "Non-trainable params: 38,052,676\n",
            "__________________________________________________________________________________________________\n",
            "Train on 115812 samples, validate on 15680 samples\n",
            "Epoch 1/3\n",
            "115812/115812 [==============================] - 2422s 21ms/sample - loss: 0.8744 - custom_acc_orig_tokens: 0.6585 - custom_acc_orig_non_other_tokens: 0.6585 - val_loss: 0.7921 - val_custom_acc_orig_tokens: 0.7070 - val_custom_acc_orig_non_other_tokens: 0.7070\n",
            "Epoch 2/3\n",
            "115812/115812 [==============================] - 2417s 21ms/sample - loss: 0.7170 - custom_acc_orig_tokens: 0.7344 - custom_acc_orig_non_other_tokens: 0.7344 - val_loss: 0.3727 - val_custom_acc_orig_tokens: 0.9130 - val_custom_acc_orig_non_other_tokens: 0.9130\n",
            "Epoch 3/3\n",
            "115812/115812 [==============================] - 2401s 21ms/sample - loss: 0.2368 - custom_acc_orig_tokens: 0.9504 - custom_acc_orig_non_other_tokens: 0.9504 - val_loss: 0.2433 - val_custom_acc_orig_tokens: 0.9493 - val_custom_acc_orig_non_other_tokens: 0.9493\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f69477d14a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3taOiZz-3xV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(OUTPUT_DIR + 'bert_bio_architecture.json', 'w') as f:\n",
        "    f.write(model.to_json())\n",
        "\n",
        "model.save_weights(OUTPUT_DIR + 'bert_bio_model_weights.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elvcOutuIZVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgxhGbL1vRar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ab71c41-5179-417a-ec5f-2ba7d0da2489"
      },
      "source": [
        "score = model.evaluate(bert_inputs_test_k, labels_test_k)\n",
        "print(f'Model loss: {score}')\n",
        "#     bert_inputs_train_k, \n",
        "#     {\"ner\":labels_train_k}"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12217/12217 [==============================] - 95s 8ms/sample - loss: 0.2025 - custom_acc_orig_tokens: 0.9588 - custom_acc_orig_non_other_tokens: 0.9588\n",
            "Model loss: [0.20248660124033954, 0.958773, 0.958773]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1q5MSOU1BwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(bert_inputs_test_k).argmax(2)\n",
        "yh = labels_test_k\n",
        "fyh_test, fpr_test = y_pred.flatten()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw14t_xXrzlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accuracy_score(fyh_test, fpr_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHjlz_xEtoF3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "569e8aaf-3caf-4b84-b893-748bda21d064"
      },
      "source": [
        "score = model.evaluate(bert_inputs_bios_k, labels_bios_k)\n",
        "print(f'Model loss: {score}')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "708/708 [==============================] - 6s 8ms/sample - loss: 0.5804 - custom_acc_orig_tokens: 0.8585 - custom_acc_orig_non_other_tokens: 0.8585\n",
            "Model loss: [0.5804318546575341, 0.85846925, 0.85846925]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLrQ6lcABZy4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b90f88a8-0083-4b43-f70f-ddcb07a59bd2"
      },
      "source": [
        "print('Training accuracy Onto TEST : ', accuracy_score(fyh_test, fpr_test))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy Onto TEST :  0.9245109273962512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8u8dr6NsQmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(classification_report(fyh_test, fpr_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs5mbvvKsVTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(bert_inputs_bios_k).argmax(2)\n",
        "yh = labels_bios_k\n",
        "fyh_bios, fpr_bios = y_pred.flatten(), yh.flatten()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM92hfD5s0cU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "557b4367-09ef-41c6-86e1-20f69526cdff"
      },
      "source": [
        "print('Training accuracy DEF14A : ', accuracy_score(y_pred.flatten(), yh.flatten()))\n",
        "# print('Training confusion matrix:')\n",
        "cm = confusion_matrix(fyh_test, fpr_test)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy DEF14A :  0.8252370863599677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sDmpgIss1lF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classification_report(fyh_bios, fpr_bios))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g3GspVUs7L9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}