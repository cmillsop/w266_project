{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "to_BILOU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwupMwKTpD-r",
        "colab_type": "code",
        "outputId": "629b47b2-dc80-40fb-c752-33c6d6366b87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from typing import Callable, List, Set, Tuple, TypeVar, Optional\n",
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "import gzip\n",
        "drive.mount('/content/drive/')\n",
        "__dir__ = '/content/drive/My Drive/Colab Notebooks/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOiIEjQlppgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InvalidTagSequence(Exception):\n",
        "    def __init__(self, tag_sequence=None):\n",
        "        super().__init__()\n",
        "        self.tag_sequence = tag_sequence\n",
        "\n",
        "    def __str__(self):\n",
        "        return ' '.join(self.tag_sequence)\n",
        "\n",
        "def to_bioul(tag_sequence: List[str], encoding: str = \"IOB1\") -> List[str]:\n",
        "    \"\"\"\n",
        "    Given a tag sequence encoded with IOB1 labels, recode to BIOUL.\n",
        "    In the IOB1 scheme, I is a token inside a span, O is a token outside\n",
        "    a span and B is the beginning of span immediately following another\n",
        "    span of the same type.\n",
        "    In the BIO scheme, I is a token inside a span, O is a token outside\n",
        "    a span and B is the beginning of a span.\n",
        "    Parameters\n",
        "    ----------\n",
        "    tag_sequence : ``List[str]``, required.\n",
        "        The tag sequence encoded in IOB1, e.g. [\"I-PER\", \"I-PER\", \"O\"].\n",
        "    encoding : `str`, optional, (default = ``IOB1``).\n",
        "        The encoding type to convert from. Must be either \"IOB1\" or \"BIO\".\n",
        "    Returns\n",
        "    -------\n",
        "    bioul_sequence: ``List[str]``\n",
        "        The tag sequence encoded in IOB1, e.g. [\"B-PER\", \"L-PER\", \"O\"].\n",
        "    \"\"\"\n",
        "\n",
        "    if not encoding in {\"IOB1\", \"BIO\"}:\n",
        "        raise ConfigurationError(f\"Invalid encoding {encoding} passed to 'to_bioul'.\")\n",
        "    # pylint: disable=len-as-condition\n",
        "\n",
        "    def replace_label(full_label, new_label):\n",
        "        # example: full_label = 'I-PER', new_label = 'U', returns 'U-PER'\n",
        "        parts = list(full_label.partition('-'))\n",
        "        parts[0] = new_label\n",
        "        return ''.join(parts)\n",
        "\n",
        "    def pop_replace_append(in_stack, out_stack, new_label):\n",
        "        # pop the last element from in_stack, replace the label, append\n",
        "        # to out_stack\n",
        "        tag = in_stack.pop()\n",
        "        new_tag = replace_label(tag, new_label)\n",
        "        out_stack.append(new_tag)\n",
        "\n",
        "    def process_stack(stack, out_stack):\n",
        "        # process a stack of labels, add them to out_stack\n",
        "        if len(stack) == 1:\n",
        "            # just a U token\n",
        "            pop_replace_append(stack, out_stack, 'U')\n",
        "        else:\n",
        "            # need to code as BIL\n",
        "            recoded_stack = []\n",
        "            pop_replace_append(stack, recoded_stack, 'L')\n",
        "            while len(stack) >= 2:\n",
        "                pop_replace_append(stack, recoded_stack, 'I')\n",
        "            pop_replace_append(stack, recoded_stack, 'B')\n",
        "            recoded_stack.reverse()\n",
        "            out_stack.extend(recoded_stack)\n",
        "\n",
        "\n",
        "    # Process the tag_sequence one tag at a time, adding spans to a stack,\n",
        "    # then recode them.\n",
        "    bioul_sequence = []\n",
        "    stack: List[str] = []\n",
        "\n",
        "    for label in tag_sequence:\n",
        "        # need to make a dict like\n",
        "        # token = {'token': 'Matt', \"labels\": {'conll2003': \"B-PER\"}\n",
        "        #                   'gold': 'I-PER'}\n",
        "        # where 'gold' is the raw value from the CoNLL data set\n",
        "\n",
        "        if label == 'O' and len(stack) == 0:\n",
        "            bioul_sequence.append(label)\n",
        "        elif label == 'O' and len(stack) > 0:\n",
        "            # need to process the entries on the stack plus this one\n",
        "            process_stack(stack, bioul_sequence)\n",
        "            bioul_sequence.append(label)\n",
        "        elif label[0] == 'I':\n",
        "            # check if the previous type is the same as this one\n",
        "            # if it is then append to stack\n",
        "            # otherwise this start a new entity if the type\n",
        "            # is different\n",
        "            if len(stack) == 0:\n",
        "                if encoding == \"BIO\":\n",
        "                    raise InvalidTagSequence(tag_sequence)\n",
        "                stack.append(label)\n",
        "            else:\n",
        "                # check if the previous type is the same as this one\n",
        "                this_type = label.partition('-')[2]\n",
        "                prev_type = stack[-1].partition('-')[2]\n",
        "                if this_type == prev_type:\n",
        "                    stack.append(label)\n",
        "                else:\n",
        "                    if encoding == \"BIO\":\n",
        "                        raise InvalidTagSequence(tag_sequence)\n",
        "                    # a new entity\n",
        "                    process_stack(stack, bioul_sequence)\n",
        "                    stack.append(label)\n",
        "        elif label[0] == 'B':\n",
        "            if len(stack) > 0:\n",
        "                process_stack(stack, bioul_sequence)\n",
        "            stack.append(label)\n",
        "        else:\n",
        "            raise InvalidTagSequence(tag_sequence)\n",
        "\n",
        "    # process the stack\n",
        "    if len(stack) > 0:\n",
        "        process_stack(stack, bioul_sequence)\n",
        "\n",
        "    return bioul_sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdKfR6hZpPkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_onto_data_vectors(data_type):\n",
        "  data_type_dict = {'test' : '/content/drive/My Drive/Colab Notebooks/data/onto.test.ner',\n",
        "                   'dev' : '/content/drive/My Drive/Colab Notebooks/data/onto.development.ner',\n",
        "                   'train' : '/content/drive/My Drive/Colab Notebooks/data/onto.train.ner'}\n",
        "  X,y = [], []\n",
        "  sentence = []\n",
        "  ner_sent = []\n",
        "  \n",
        "\n",
        "  with open(data_type_dict[data_type], 'r') as f:\n",
        "      for line in f.readlines():\n",
        "\n",
        "          if line !='\\n':\n",
        "  #            print(line.split('\\t'))\n",
        "              word, *_, ner = line.split('\\t')\n",
        "  #            print(ner)\n",
        "              sentence.append(word)\n",
        "              ner_sent.append(ner.rstrip())\n",
        "\n",
        "          else:\n",
        "              X.append(sentence)\n",
        "              y.append(ner_sent)\n",
        "              sentence = []\n",
        "              ner_sent=[]\n",
        "              continue\n",
        "  \n",
        "  return X[1:], y[1:] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PyaQsbLqB7A",
        "colab_type": "code",
        "outputId": "beee0cfb-521a-44bf-9c04-1b4d464d4afb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "X_train, y_train = create_onto_data_vectors('train')\n",
        "y_train_bilou = []\n",
        "for seq in y_train:\n",
        "  seq_bilou = to_bioul(seq, 'BIO')\n",
        "  y_train_bilou.append(seq_bilou)\n",
        "\n",
        "print(y_train_bilou[:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'L-ORG', 'O'], ['B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'L-WORK_OF_ART'], ['O', 'O', 'O', 'B-LOC', 'L-LOC', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'L-WORK_OF_ART', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'L-WORK_OF_ART', 'O', 'O', 'O', 'O', 'O']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzYnlRFtrk50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def df_and_csv(X, y, dataset):\n",
        "  data_type_dict = {'test' : '/content/drive/My Drive/Colab Notebooks/data/onto_notes_test_bilou.csv',\n",
        "                 'dev' : '/content/drive/My Drive/Colab Notebooks/data/onto_notes_dev_bilou.csv',\n",
        "                 'train' : '/content/drive/My Drive/Colab Notebooks/data/onto_notes_train_bilou.csv'}\n",
        "  \n",
        "  pd_bilou = pd.DataFrame([X,y]).T\n",
        "  pd_bilou.columns=['X_dev', 'y_dev']\n",
        "  pd_bilou.head()\n",
        "  pd_bilou.to_csv(data_type_dict[dataset])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHFyhOazu2pl",
        "colab_type": "code",
        "outputId": "71d36a0c-bec7-46e9-a2a5-863e3887e21f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "X_train, y_train = create_onto_data_vectors('train')\n",
        "y_train_bilou = []\n",
        "for seq in y_train:\n",
        "  seq_bilou = to_bioul(seq, 'BIO')\n",
        "  y_train_bilou.append(seq_bilou)\n",
        "\n",
        "print(y_train_bilou[:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['In', '1940', ',', 'the', 'German', 'army', 'invaded', 'and', 'occupied', 'Czechoslovakia', ',', 'Poland', ',', 'the', 'Netherlands', ',', 'Belgium', ',', 'and', 'France', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuVKlcPfzi1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_dev, y_dev = create_onto_data_vectors('dev')\n",
        "y_dev_bilou = []\n",
        "for seq in y_dev:\n",
        "  seq_bilou = to_bioul(seq, 'BIO')\n",
        "  y_dev_bilou.append(seq_bilou)\n",
        "\n",
        "  \n",
        "df_and_csv(X_dev, y_dev_bilou, 'dev')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}